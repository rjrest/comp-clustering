{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Outline of future code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The complete project should contain a series of scripts that start from these inputs:\n",
      "#     * csv from internal SQL query results  (confidential - can't put on Github)\n",
      "# \n",
      "# A. And produces a series of outputs:\n",
      "#   * clean/coded csv's (on Github)\n",
      "#   * Feature engineering  (20-45 derived measurements)\n",
      "#     * output: measurement csvs to ./data/cleaned/  (ok on Github)\n",
      "#     * plot:   histograms to ./data/vis/  (ok on Github)\n",
      "#   * Feature normalization:\n",
      "#     * output:  normalized measurement csv's (on Github)\n",
      "#\n",
      "# B. Assembles outputs:\n",
      "#   * JOIN ON competitionId to each normalized measurement table\n",
      "#\n",
      "# C. Unsupervised analysis:\n",
      "#   * (?) Performs PCA analysis\n",
      "#   * KMeans clustering\n",
      "#\n",
      "# D. Visualizer\n",
      "#\n",
      "# E. Analytical write-up  (confidential - present to instructors & Kaggle team)\n",
      "\n",
      "\n",
      "# Part A\n",
      "\n",
      "# Data_cleaner:\n",
      "#    1. ingest data/raw/Competitions csv\n",
      "#       1. rename columns to X1, X2, X3 etc.\n",
      "#       2. drop unneeded columns\n",
      "#    2. OUTPUT:  data/clean/competitions_clean.csv\n",
      "#    3. Munge data (missing values)\n",
      "#    4. Repeat 1-3 for all other tables\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Class data_cleaner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: RJ Ramey <rj.github@garimeme.com>\n",
      "# License: (c) 2014 by RJ Ramey. All rights reserved. No license is given at this time."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup and Initialization\n",
      "from datetime import datetime\n",
      "from time import time\n",
      "import os.path\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
      "\n",
      "# IO / Load data\n",
      "\n",
      "def modification_date(filename):\n",
      "    \"\"\" return the modified date from a filename \"\"\"\n",
      "    t = os.path.getmtime(filename)\n",
      "    return datetime.fromtimestamp(t)\n",
      "\n",
      "def printhelp():\n",
      "    \"\"\" prints expected inputs to the console \"\"\"\n",
      "    \n",
      "    print \"data_cleaner expects certain (non-public) files in /data/raw/ :\"\n",
      "    print \n",
      "    print \"Prerequisite files: \"\n",
      "    print \"  - QueryResults1784.csv\"\n",
      "    print \"  - QueryResults1790.csv\"\n",
      "    print \"  - QueryResults1792.csv\"\n",
      "##  ...\n",
      "    print\n",
      "    print \"where:\"\n",
      "    print \"  4-trailing digits xxxx.csv indicate the output of a SQL Explorer query found at https://sql.kaggle.com/kaggle/xxxx/\"\n",
      "\n",
      "# Load Competitions table (1784)\n",
      "def clean1784(filename):\n",
      "    \"\"\" ingests, cleans and munges columns from /raw/QueryResults1784.csv \n",
      "    \n",
      "    Returns:\n",
      "      coded_legend: dict of X1,X2,...,X23 column names to their original name from the raw query results\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_Competitions = pd.read_csv(filename,header=0,index_col=0)\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1784.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "        \n",
      "    \n",
      "    # Code the feature names for public storage on Github\n",
      "    print \"Masking feature names from raw data ...\"\n",
      "    t00 = time()\n",
      "    coded_names = [ 'X' + str(i) for i in range (1, len(df_Competitions.columns) + 1)]\n",
      "            # save the dictionary for future reversal, will return this value from function\n",
      "    coded_legend = { code: orig for code, orig in (zip(coded_names, df_Competitions.columns))}\n",
      "    \n",
      "    df_Competitions.columns = coded_names\n",
      "    \n",
      "    \n",
      "    # Data Munging\n",
      "    \n",
      "    # Convert X17 {3: 3, all others : 1}\n",
      "    print \"Flatten feature X17 to values {1,3} ...\"\n",
      "    df_Competitions.loc[ df_Competitions.X17 <> 3, 'X17' ] = 1 \n",
      "    \n",
      "    # Create future JOIN table of foreign keys\n",
      "    print \"Saving future JOIN table:  /data/cleaned/foreignkeys.csv ...\"\n",
      "    df_ForeignKeys = df_Competitions[['X10','X12','X13','X16','X17','X18','X22','X23']]\n",
      "    df_ForeignKeys.to_csv('./data/cleaned/foreignkeys.csv') \n",
      "    \n",
      "    print \"Force column type of boolean and date fields ...\"\n",
      "      # Convert X23 to bool, {null: 0, notnull: 1}\n",
      "    df_Competitions['Is_X23'] = df_Competitions.X23.map(lambda x: x/x)     # this evaluates to 1\n",
      "    df_Competitions.loc[ df_Competitions.Is_X23.isnull(), 'Is_X23' ] = 0   # sets remaining 0\n",
      "    coded_legend['Is_X23'] = str('Is' + coded_legend['X23'][0:20])         # save meaning of IsX23 in coded names\n",
      "    \n",
      "    # Convert boolean columns to 1/0\n",
      "    for cn in list(df_Competitions.columns[df_Competitions.dtypes.map(lambda x: x=='bool')]):\n",
      "        df_Competitions[cn] = df_Competitions[cn].astype(int)\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_Competitions['X5'] = pd.to_datetime(df_Competitions.X5, utc=True)   \n",
      "    df_Competitions['X6'] = pd.to_datetime(df_Competitions.X6, utc=True)   \n",
      "    df_Competitions['X7'] = pd.to_datetime(df_Competitions.X7, utc=True)   \n",
      "    df_Competitions['X8'] = pd.to_datetime(df_Competitions.X8, utc=True)   \n",
      "    df_Competitions['X19'] = pd.to_datetime(df_Competitions.X19, utc=True)    \n",
      "    \n",
      "    print \"Checking for valid dates:\"\n",
      "    t0 = time()\n",
      "    \n",
      "    # Check where X5 is missing; assume it to = X6\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X5.isnull() == True])\n",
      "    if flags > 0:\n",
      "        print \"    INFO: filling %d missing dates ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X5.isnull() == True), 'X5' ] = df_Competitions.X6\n",
      "    \n",
      "    # Check where dates should be previous in time to the subsequent one; force it so\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X6 > df_Competitions.X7])\n",
      "    if flags > 0:\n",
      "        print \"    INFO: found %d questionable dates relative to Launch. Making assumptions ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X6 > df_Competitions.X7), 'X6' ] = df_Competitions.X7\n",
      "        \n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ (df_Competitions.X6.isnull() == False) & (df_Competitions.X5 > df_Competitions.X6)])\n",
      "    if flags > 0:\n",
      "        print \"    INFO: found %d more questionable dates. Making assumptions ...\" % flags\n",
      "        \n",
      "        # calculate to find value of X5 that are off by almost exactly 1 year\n",
      "        df_Competitions['X5_X6'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X5 - df_Competitions.X6),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "        \n",
      "        # if seems close to 1 year off, assume human error and subtract 1 year\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) , 'X5' ] = pd.to_datetime(df_Competitions.X5) - np.timedelta64(366,'D')\n",
      "            \n",
      "        # but if not almost exactly 1 year, then just force to same as X6\n",
      "        if len(df_Competitions[df_Competitions.X5_X6 >= 367]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 >= 367), 'X5' ] = df_Competitions.X6\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300), 'X5' ] = df_Competitions.X6\n",
      "        \n",
      "        # drop the temporary calculation column\n",
      "        df_Competitions = df_Competitions.drop(['X5_X6'], axis=1)\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"    ::done in %fs\" % duration\n",
      "    \n",
      "    \n",
      "    # Additional feature engineering\n",
      "    # do date arithmetic, converts to ns, calculate back to days\n",
      "    origindate = datetime(2010, 1, 1)\n",
      "    analysisdate = modification_date(filename)\n",
      "    analysisdate64 = np.datetime64(analysisdate)\n",
      "    todaynow64 = np.datetime64(datetime.utcnow())\n",
      "    \n",
      "    print \"Today is \", np.datetime_as_string(todaynow64)[0:10]\n",
      "    print \"QueryResults have timestamp of \", np.datetime_as_string(analysisdate64)[0:10]\n",
      "    print \"Creating new time measurements from datestamps ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    df_Competitions['LaunchDaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_Competitions['Day3'] = df_Competitions.LaunchDaySeq + 3\n",
      "    df_Competitions['Day7'] = df_Competitions.LaunchDaySeq + 7\n",
      "    df_Competitions['Day14'] = df_Competitions.LaunchDaySeq + 14\n",
      "    df_Competitions['Day30'] = df_Competitions.LaunchDaySeq + 30\n",
      "    \n",
      "    df_Competitions['DeadlineDaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    \n",
      "    df_Competitions['InstructorPrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X6 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['PrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationInt'] = (df_Competitions.DeadlineDaySeq - df_Competitions.LaunchDaySeq)\n",
      "    \n",
      "    df_Competitions['DayOfYear'] = pd.to_datetime(df_Competitions.X7.values).dayofyear\n",
      "    df_Competitions['DayOfWeek'] = pd.to_datetime(df_Competitions.X7.values).dayofweek + 1   # Monday = 1, Sunday = 7\n",
      "    \n",
      "    df_Competitions['Postprocess_sec'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X19 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 1000000000\n",
      "    \n",
      "    df_Competitions['AgeInDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['RemainingDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    # Delete all rows where comp is too young to analyze\n",
      "    #   To use here, a comp needs to be >30 days old (Today - X7 > 30d)\n",
      "    #   and need to be < 30 days out to the final Deadline (Today - X8 > -30)\n",
      "    \n",
      "    print \"Dropping competitions that are too young to analyze ...\"\n",
      "    df_Competitions = df_Competitions[ ((df_Competitions.AgeInDays > 30) & (df_Competitions.RemainingDays > -30))]\n",
      "    \n",
      "    # Drop rows for strange data (these dont seem to be real comps)\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 2553]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 2554]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 3867]\n",
      "    # Drop rows for comps less than 1.5 days duration (note: hackathons will be dropped)\n",
      "    df_Competitions = df_Competitions[df_Competitions.DurationDays > 1.5]\n",
      "    \n",
      "    unique_comps = np.unique(df_Competitions.index.values)\n",
      "    print\n",
      "    print \"%d competitions are remaining for analysis\" % len(unique_comps)\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_Competitions = df_Competitions.drop(['X10','X12','X13','X16','X17','X18','X22','X23'], axis=1)   # the foreign key columns\n",
      "    df_Competitions = df_Competitions.drop(['X2','X3','X4'], axis=1)                                    # the descriptive text\n",
      "    df_Competitions = df_Competitions.drop(['X19'], axis=1)                        # Unneeded date column (now measured in Postprocess_sec)\n",
      "    df_Competitions = df_Competitions.drop(['AgeInDays','RemainingDays'], axis=1)  # Temp columns were needed only to filter out one-time\n",
      "    \n",
      "    \n",
      "    # !!!!!  TEMP   !!!!!!\n",
      "    # for now, store a many-column readable version\n",
      "    print\n",
      "    try:\n",
      "        df_Competitions.to_csv('./data/cleaned/comps_full.csv') \n",
      "        print \"=  Temp output: /data/cleaned/comps_full.csv  (TODO: delete this step)\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/comps_full.csv\"\n",
      "        print\n",
      "    \n",
      "    \n",
      "    # Visualizations for exploration\n",
      "    def visualizeme(df):\n",
      "        \n",
      "        t0 = time()\n",
      "        \n",
      "        # Instantiate figure0 for histograms\n",
      "        fig = plt.figure(num=0, figsize=(15,22))\n",
      "        fig.suptitle('Histograms of new features', fontsize=14)\n",
      "        ax1 = fig.add_subplot(4,1,1)\n",
      "        ax2 = fig.add_subplot(4,1,2)\n",
      "        ax3 = fig.add_subplot(4,1,3)\n",
      "        ax4 = fig.add_subplot(4,1,4)\n",
      "        \n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"count\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #hist 1\n",
      "            range_of_1_std = df.DurationInt.std() + df.DurationInt.mean()\n",
      "            ax1.hist(df.DurationInt.values, bins=15, range=(0,range_of_1_std), color='r',alpha=0.5)\n",
      "            ax1.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "            #hist 2\n",
      "            range_of_1_std = df.PrepDays.std() + df.PrepDays.mean()\n",
      "            ax2.hist(df.PrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.5)\n",
      "            ax2.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #hist 3\n",
      "            range_of_1_std = df.InstructorPrepDays.std() + df.InstructorPrepDays.mean()\n",
      "            ax3.hist(df.InstructorPrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.25)\n",
      "            ax3.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #hist 4\n",
      "            range_of_1_std = df.Postprocess_sec.std() + df.Postprocess_sec.mean()\n",
      "            ax4.hist(df.Postprocess_sec.values, bins=25, range=(0,range_of_1_std), color='g',alpha=0.8)\n",
      "            ax4.set_xlabel(\"(sec)\", fontsize=10)\n",
      "            ax4.set_title('private leaderboard Postprocess_sec', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting histograms\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_histograms.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_histograms.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_histograms.png\"\n",
      "            print\n",
      "        \n",
      "        t0 = time()\n",
      "        # Instantiate figure 1 for boxplots\n",
      "        fig = plt.figure(num=1, figsize=(15,8))\n",
      "        fig.suptitle('Outliers in Preparation days', fontsize=14)\n",
      "        ax1 = fig.add_subplot(1,3,1)\n",
      "        ax2 = fig.add_subplot(1,3,2)\n",
      "        ax3 = fig.add_subplot(1,3,3)\n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"days\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #boxplot 1\n",
      "            ax2.boxplot(df.PrepDays.values, sym='go')\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #boxplot 2\n",
      "            ax3.boxplot(df.InstructorPrepDays.values, sym='bo')\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #boxplot 3\n",
      "            ax1.boxplot(df.DurationInt.values, sym='ro')\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting boxplots\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_outliers.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_outliers.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_outliers.png\"\n",
      "            print\n",
      "    \n",
      "    visualizeme(df_Competitions)\n",
      "    \n",
      "    # Drop remaining columns no longer needed\n",
      "    df_Competitions = df_Competitions.drop(['X1','X5','X6','X7','X8','X9','X28','X29'], axis=1)\n",
      "    # ... more to come? ... \n",
      "    \n",
      "    # Write to /data/cleaned folder\n",
      "    try:\n",
      "        df_Competitions.to_csv('./data/cleaned/comps.csv') \n",
      "        print \"=  Clean output: /data/cleaned/comps.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/comps.csv\"\n",
      "        print\n",
      "    \n",
      "    # Reclaim memory\n",
      "    df_Competitions = []\n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return coded_legend\n",
      "    \n",
      "codenames = clean1784(\"data/raw/QueryResults1784.csv\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1784.csv\n",
        "________________________________________________________________________________\n",
        "Masking feature names from raw data ...\n",
        "Flatten feature X17 to values {1,3} ...\n",
        "Saving future JOIN table:  /data/cleaned/foreignkeys.csv ...\n",
        "Force column type of boolean and date fields ...\n",
        "Checking for valid dates:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    INFO: filling 54 missing dates ...\n",
        "    INFO: found 5 questionable dates relative to Launch. Making assumptions ...\n",
        "    INFO: found 8 more questionable dates. Making assumptions ...\n",
        "    ::done in 0.031746s\n",
        "Today is  2014-05-08\n",
        "QueryResults have timestamp of  2014-05-01\n",
        "Creating new time measurements from datestamps ...\n",
        "  ::done in 0.029249s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Dropping competitions that are too young to analyze ...\n",
        "\n",
        "359 competitions are remaining for analysis\n",
        "\n",
        "=  Temp output: /data/cleaned/comps_full.csv  (TODO: delete this step)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "=  Vis Output: /data/vis/features1_histograms.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 1.637979s\n",
        "=  Vis Output: /data/vis/features1_outliers.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 0.597725s\n",
        "=  Clean output: /data/cleaned/comps.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "File done in 2.705400s\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%whos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Variable             Type                          Data/Info\n",
        "------------------------------------------------------------\n",
        "FormatStrFormatter   type                          <class 'matplotlib.ticker.FormatStrFormatter'>\n",
        "MultipleLocator      type                          <class 'matplotlib.ticker.MultipleLocator'>\n",
        "clean1784            function                      <function clean1784 at 0x1062ee0c8>\n",
        "codenames            dict                          n=34\n",
        "datetime             type                          <type 'datetime.datetime'>\n",
        "modification_date    function                      <function modification_date at 0x10cedfde8>\n",
        "np                   module                        <module 'numpy' from '/Us<...>ages/numpy/__init__.pyc'>\n",
        "os                   module                        <module 'os' from '/Users<...>ts/lib/python2.7/os.pyc'>\n",
        "pd                   module                        <module 'pandas' from '/U<...>ges/pandas/__init__.pyc'>\n",
        "plt                  module                        <module 'matplotlib.pyplo<...>s/matplotlib/pyplot.pyc'>\n",
        "printhelp            function                      <function printhelp at 0x109d94c08>\n",
        "time                 builtin_function_or_method    <built-in function time>\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load RuleAccepters (1790)\n",
      "def clean1790(filename):\n",
      "    \"\"\" ingests, cleans and munges columns from /raw/QueryResults1790.csv \"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_RuleAccepters = pd.read_csv(filename,header=0)\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1790.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_RuleAccepters['LogDate'] = pd.to_datetime(df_RuleAccepters.LogDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_RuleAccepters['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_RuleAccepters.LogDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    # remove null CompetitionIds\n",
      "    df_RuleAccepters = df_RuleAccepters[ df_RuleAccepters.CompetitionId.isnull() == False ]\n",
      "    df_RuleAccepters.sort(['CompetitionId','LogDate'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    \n",
      "    print \"Contains data for %d competitions,\" % len(np.unique(df_RuleAccepters.CompetitionId))\n",
      "    print \"  constraining to %d competitions under analysis\" % len(unique_comps)\n",
      "    print \"  Calculating cumulative total of Accepters per day ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in unique_comps:\n",
      "        operate = df_RuleAccepters.iloc[:,[1,3,2]][ (df_RuleAccepters.CompetitionId == c) ].set_index('DaySeq')\n",
      "        cumul = 0\n",
      "        for i in operate.index:\n",
      "            cumul = cumul + operate.xs(i,axis=0,copy=False)['RuleAccepted']\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    duration = time() - t0\n",
      "    print \"::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    \n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulAccepted']).to_csv('./data/cleaned/rules_accepters.csv') \n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/rules_accepters.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/rules_accepters.csv\"\n",
      "        print\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([3,])\n",
      "    operate = []\n",
      "    df_RuleAccepters = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    \n",
      "clean1790(\"data/raw/QueryResults1790.csv\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "==== Cleaning QueryResults1790.csv ====\n",
        "Contains data for 439 competitions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Constraining to 359 competitions under analysis\n",
        "Calculating cumulative total of Accepters per day ...\n",
        "...done in 11.276718s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output : /data/cleaned/rules_accepters.csv\n",
        "File done in 11.827179s\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Forums table (1792)\n",
      "def clean1792(filename):\n",
      "    \"\"\" ingests, cleans and munges columns from /raw/QueryResults1792.csv \"\"\"\n",
      "    \n",
      "    t00 = time()\n",
      "    try:\n",
      "        df_Forums = pd.read_csv(filename,header=0,usecols=[0,1,2,3,4,5,6,7],converters={'Message': lambda x: len(str(x))})\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,4])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1792.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    #print \"Force column type of date fields ...\"\n",
      "    #df_Forums['PostDate'] = pd.to_datetime(df_Forums.PostDate, utc=True)  \n",
      "    \n",
      "    # Join to get CompId\n",
      "    df_Forums = pd.merge(df_keys, df_Forums, left_on='X16', right_on='ForumId', how='inner')\n",
      "    df_Forums.rename(columns={'Message':'MessageLen'}, inplace=True)\n",
      "                      \n",
      "    # Sum separately on forum Topics and Messages\n",
      "    \n",
      "    # Sort and count Forum Messages\n",
      "    t0 = time()\n",
      "    print \"Counting Forum messages per day...\"\n",
      "    df_ForumMg = df_Forums.drop(['X16','Name','Name.1','ForumId','ForumTopicId','FlaggedCount'], axis=1)\n",
      "    df_ForumMg['PostDate'] = df_ForumMg.PostDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_ForumMg.set_index(['Id','ForumMessageId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    df_ForumMg.sort(['Id','ForumMessageId'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    \n",
      "    # create a DataFrameGroupBy:\n",
      "    by_comp_by_date = df_ForumMg.groupby(['Id','PostDate'])\n",
      "    try:\n",
      "        # Get counts via function size()\n",
      "        df_ForumMgCount = by_comp_by_date.size().to_frame().to_csv('./data/cleaned/forum_msg.csv')\n",
      "        df_ForumMgCount = []\n",
      "        print \"=  Clean output: /data/cleaned/forum_msg.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/forum_msg.csv\"\n",
      "        print\n",
      "\n",
      "clean1792(\"data/raw/QueryResults1792.csv\")\n",
      "\n",
      "def testing:\n",
      "    # Sort and count Forum Topics\n",
      "    t0 = time()\n",
      "    print \"Counting Forum topics per day...\"\n",
      "    df_ForumTp = df_Forums.drop(['X16','Name','Name.1','MessageLen','FlaggedCount'], axis=1)\n",
      "          #df_ForumTp.set_index(['Id','ForumId','ForumTopicId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    df_ForumTp['PostDate'] = df_ForumTp.PostDate.map(lambda x:x[0:10])  # dumb down timestamp to just day\n",
      "    df_ForumTp['PostDate'] = pd.to_datetime(df_ForumTp.PostDate, utc=True)\n",
      "    \n",
      "        \n",
      "    \n",
      "# Reclaim memory\n",
      "df_keys = []\n",
      "df_ForumMgCount = []\n",
      "#df_ForumMg = []\n",
      "#by_comp_by_date = []\n",
      "#df_Forums = []\n",
      "#df_ForumTp = []\n",
      "\n",
      "duration = time() - t00\n",
      "print \"File done in %fs\" % duration\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "==== Cleaning QueryResults1790.csv ====\n",
        "Counting Forum messages per day...\n",
        "=  Clean output: /data/cleaned/forum_msg.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    done in 0.169630s\n",
        "Counting Forum topics per day...\n",
        "File done in 0.554783s\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_ForumMg.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>Id</th>\n",
        "      <th>ForumMessageId</th>\n",
        "      <th>PostDate</th>\n",
        "      <th>MessageLen</th>\n",
        "      <th>DaySeq</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Id</th>\n",
        "      <th>ForumMessageId</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"3\" valign=\"top\">2408</th>\n",
        "      <th>28</th>\n",
        "      <td> 2408</td>\n",
        "      <td> 28</td>\n",
        "      <td>2010-05-12</td>\n",
        "      <td> 1584</td>\n",
        "      <td> 131</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 2408</td>\n",
        "      <td> 29</td>\n",
        "      <td>2010-05-12</td>\n",
        "      <td> 1838</td>\n",
        "      <td> 131</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td> 2408</td>\n",
        "      <td> 30</td>\n",
        "      <td>2010-05-12</td>\n",
        "      <td>  255</td>\n",
        "      <td> 131</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"2\" valign=\"top\">2435</th>\n",
        "      <th>1 </th>\n",
        "      <td> 2435</td>\n",
        "      <td>  1</td>\n",
        "      <td>2010-04-28</td>\n",
        "      <td> 2413</td>\n",
        "      <td> 117</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 2435</td>\n",
        "      <td>  2</td>\n",
        "      <td>2010-04-29</td>\n",
        "      <td>  544</td>\n",
        "      <td> 118</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "                       Id  ForumMessageId   PostDate  MessageLen  DaySeq\n",
        "Id   ForumMessageId                                                     \n",
        "2408 28              2408              28 2010-05-12        1584     131\n",
        "     29              2408              29 2010-05-12        1838     131\n",
        "     30              2408              30 2010-05-12         255     131\n",
        "2435 1               2435               1 2010-04-28        2413     117\n",
        "     2               2435               2 2010-04-29         544     118\n",
        "\n",
        "[5 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# testing loop for average length of message per day\n",
      "origindate = datetime(2010, 1, 1)\n",
      "\n",
      "df_ForumMg['PostDate'] = pd.to_datetime(df_ForumMg.PostDate, utc=True)  \n",
      "df_ForumMg['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                              (df_ForumMg.PostDate - origindate),\n",
      "                                              unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "\n",
      "# create a DataFrameGroupBy:\n",
      "by_comp = df_ForumMg.groupby(['Id'])\n",
      "by_comp.mean().tail()\n",
      "#df_ForumMgAvgLength = by_comp_by_date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>ForumMessageId</th>\n",
        "      <th>MessageLen</th>\n",
        "      <th>DaySeq</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Id</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>3871</th>\n",
        "      <td> 40639.375000</td>\n",
        "      <td>  334.375000</td>\n",
        "      <td> 1538.125000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3872</th>\n",
        "      <td> 41017.529412</td>\n",
        "      <td> 3182.235294</td>\n",
        "      <td> 1544.705882</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3888</th>\n",
        "      <td> 42213.000000</td>\n",
        "      <td>  165.000000</td>\n",
        "      <td> 1566.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3897</th>\n",
        "      <td> 42361.105263</td>\n",
        "      <td>  429.134503</td>\n",
        "      <td> 1568.491228</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3901</th>\n",
        "      <td> 43306.000000</td>\n",
        "      <td> 3554.333333</td>\n",
        "      <td> 1579.333333</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "      ForumMessageId   MessageLen       DaySeq\n",
        "Id                                            \n",
        "3871    40639.375000   334.375000  1538.125000\n",
        "3872    41017.529412  3182.235294  1544.705882\n",
        "3888    42213.000000   165.000000  1566.000000\n",
        "3897    42361.105263   429.134503  1568.491228\n",
        "3901    43306.000000  3554.333333  1579.333333\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}