{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Outline of future code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The complete project should contain a series of scripts that start from these inputs:\n",
      "#     * csv from internal SQL query results  (confidential - can't put on Github)\n",
      "# \n",
      "# A. And produces a series of outputs:\n",
      "#   * clean/coded csv's (on Github)\n",
      "#   * Feature engineering  (20-45 derived measurements)\n",
      "#     * output: measurement csvs to ./data/cleaned/  (ok on Github)\n",
      "#     * plot:   histograms to ./data/vis/  (ok on Github)\n",
      "#   * Feature normalization:\n",
      "#     * output:  normalized measurement csv's (on Github)\n",
      "#\n",
      "# B. Assembles outputs:\n",
      "#   * JOIN ON competitionId to each normalized measurement table\n",
      "#\n",
      "# C. Unsupervised analysis:\n",
      "#   * (?) Performs PCA analysis\n",
      "#   * KMeans clustering\n",
      "#\n",
      "# D. Visualizer\n",
      "#\n",
      "# E. Analytical write-up  (confidential - present to instructors & Kaggle team)\n",
      "\n",
      "\n",
      "# Part A\n",
      "\n",
      "# Data_cleaner:\n",
      "#    1. ingest data/raw/Competitions csv\n",
      "#       1. rename columns to X1, X2, X3 etc.\n",
      "#       2. drop unneeded columns\n",
      "#    2. OUTPUT:  data/clean/competitions_clean.csv\n",
      "#    3. Munge data (missing values)\n",
      "#    4. Repeat 1-3 for all other tables\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Class data_cleaner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: RJ Ramey <rj.github@garimeme.com>\n",
      "# License: (c) 2014 by RJ Ramey. All rights reserved. \n",
      "# No license is given at this time.\n",
      "\n",
      "# Setup and Initialization\n",
      "from datetime import datetime\n",
      "from time import time\n",
      "import os.path\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
      "\n",
      "t000 = time()\n",
      "origindate = datetime(2010, 1, 1)\n",
      "\n",
      "# IO / Load data functions\n",
      "def modification_date(filename):\n",
      "    \"\"\" return the modified date from filename on disk\"\"\"\n",
      "    t = os.path.getmtime(filename)\n",
      "    return datetime.fromtimestamp(t)\n",
      "\n",
      "def printhelp():\n",
      "    \"\"\" prints expected inputs to the console \"\"\"\n",
      "    \n",
      "    print \"\u300c(\u00b0\u30d8\u00b0)  Oops: data_cleaner expects certain (non-public) files in /data/raw/ :\"\n",
      "    print \n",
      "    print \"Prerequisite files: \"\n",
      "    print \"  - QueryResults1560.csv  from   at https://sql.kaggle.com/kaggle/1560/\"\n",
      "    print \"  - QueryResults1784.csv  from   at https://sql.kaggle.com/kaggle/1784/\"\n",
      "    print \"  - QueryResults1790.csv  from   at https://sql.kaggle.com/kaggle/1790/\"\n",
      "    print \"  - QueryResults1792.csv  from   at https://sql.kaggle.com/kaggle/1792/\"\n",
      "    print \"  - QueryResults1807.csv  from   at https://sql.kaggle.com/kaggle/1807/\"\n",
      "    print \"  - QueryResults1808.csv  from   at https://sql.kaggle.com/kaggle/1808/\"\n",
      "    print \"  - QueryResults1809.csv  from   at https://sql.kaggle.com/kaggle/1809/\"\n",
      "    print \"  - QueryResults1810.csv  from   at https://sql.kaggle.com/kaggle/1810/\"\n",
      "    print \"  - QueryResults1811.csv  from   at https://sql.kaggle.com/kaggle/1811/\"\n",
      "    print \"  - QueryResults1812.csv  from   at https://sql.kaggle.com/kaggle/1812/\"\n",
      "    print \"  - QueryResults1813.csv  from   at https://sql.kaggle.com/kaggle/1813/\"\n",
      "    print \"  - QueryResults1814.csv  from   at https://sql.kaggle.com/kaggle/1814/\"\n",
      "    print \"  - QueryResults1815.csv  from   at https://sql.kaggle.com/kaggle/1815/\"\n",
      "    print \"  - QueryResults1816.csv  from   at https://sql.kaggle.com/kaggle/1816/\"\n",
      "    print \"  - QueryResults1820.csv  from   at https://sql.kaggle.com/kaggle/1820/\"\n",
      "    print \"  - QueryResults1833.csv  from   at https://sql.kaggle.com/kaggle/1833/\"\n",
      "    print \"  - QueryResults1842.csv  from   at https://sql.kaggle.com/kaggle/1842/\"\n",
      "    print\n",
      "    return\n",
      "    \n",
      "# Load Competitions table (1784)\n",
      "def clean1784(filename,visualize=True):\n",
      "    \"\"\" ingests, cleans and munges columns from /raw/QueryResults1784.csv \n",
      "    \n",
      "    Returns:\n",
      "      coded_legend: dict of X1,X2,...,X23 column names to their original name from the raw query results\n",
      "      \n",
      "      unique_comps: array of CompetitionId's contained in comps.csv\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/comps.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_Competitions = pd.read_csv(filename,header=0,index_col=0)\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1784.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "        \n",
      "    \n",
      "    # Code the feature names for public storage on Github\n",
      "    print \"Masking feature names from raw data ...\"\n",
      "    t00 = time()\n",
      "    coded_names = [ 'X' + str(i) for i in range (1, len(df_Competitions.columns) + 1)]\n",
      "            # save the dictionary for future reversal, will return this value from function\n",
      "    coded_legend = { code: orig for code, orig in (zip(coded_names, df_Competitions.columns))}\n",
      "    \n",
      "    df_Competitions.columns = coded_names\n",
      "    \n",
      "    \n",
      "    # Data Munging\n",
      "    \n",
      "    # Convert X17 {3: 3, all others : 1}\n",
      "    print \"Flatten feature X17 to values {1 or 3} ...\"\n",
      "    df_Competitions.loc[ df_Competitions.X17 <> 3, 'X17' ] = 1 \n",
      "    \n",
      "    print \"Force column type of boolean and date fields ...\"\n",
      "      # Convert X23 to bool, {null: 0, notnull: 1}\n",
      "    df_Competitions['Is_X23'] = df_Competitions.X23.map(lambda x: x/x)     # this evaluates to 1\n",
      "    df_Competitions.loc[ df_Competitions.Is_X23.isnull(), 'Is_X23' ] = 0   # sets remaining 0\n",
      "    coded_legend['Is_X23'] = str('Is' + coded_legend['X23'][0:20])         # save meaning of IsX23 in coded names\n",
      "    \n",
      "    # Convert boolean columns to 1/0\n",
      "    for cn in list(df_Competitions.columns[df_Competitions.dtypes.map(lambda x: x=='bool')]):\n",
      "        df_Competitions[cn] = df_Competitions[cn].astype(int)\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_Competitions['X5'] = pd.to_datetime(df_Competitions.X5, utc=True)   \n",
      "    df_Competitions['X6'] = pd.to_datetime(df_Competitions.X6, utc=True)   \n",
      "    df_Competitions['X7'] = pd.to_datetime(df_Competitions.X7, utc=True)   \n",
      "    df_Competitions['X8'] = pd.to_datetime(df_Competitions.X8, utc=True)   \n",
      "    df_Competitions['X19'] = pd.to_datetime(df_Competitions.X19, utc=True)    \n",
      "    \n",
      "    print \"Checking for valid dates:\"\n",
      "    t0 = time()\n",
      "    \n",
      "    # Check where X5 is missing; assume it to = X6\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X5.isnull() == True])\n",
      "    if flags > 0:\n",
      "        print \"   filling %d missing dates  (\u2299\u2026\u2299 ) ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X5.isnull() == True), 'X5' ] = df_Competitions.X6\n",
      "    \n",
      "    # Check where dates should be previous in time to the subsequent one; force it so\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X6 > df_Competitions.X7])\n",
      "    if flags > 0:\n",
      "        print \"   found %d questionable dates relative to Launch  (\u2299\u2026\u2299 ) ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X6 > df_Competitions.X7), 'X6' ] = df_Competitions.X7\n",
      "        \n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ (df_Competitions.X6.isnull() == False) & (df_Competitions.X5 > df_Competitions.X6)])\n",
      "    if flags > 0:\n",
      "        print \"   found %d more questionable dates. Making assumptions  (\u25ce_\u25ce;)  ...\" % flags\n",
      "        \n",
      "        # calculate to find value of X5 that are off by almost exactly 1 year\n",
      "        df_Competitions['X5_X6'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X5 - df_Competitions.X6),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "        \n",
      "        # if seems close to 1 year off, assume human error and subtract 1 year\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) , 'X5' ] = pd.to_datetime(df_Competitions.X5) - np.timedelta64(366,'D')\n",
      "            \n",
      "        # but if not almost exactly 1 year, then just force to same as X6\n",
      "        if len(df_Competitions[df_Competitions.X5_X6 >= 367]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 >= 367), 'X5' ] = df_Competitions.X6\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300), 'X5' ] = df_Competitions.X6\n",
      "        \n",
      "        # drop the temporary calculation column\n",
      "        df_Competitions = df_Competitions.drop(['X5_X6'], axis=1)\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"   ::done in %fs\" % duration\n",
      "    \n",
      "    \n",
      "    # Additional feature engineering\n",
      "    # do date arithmetic, converts to ns, calculate back to days\n",
      "    analysisdate = modification_date(filename)\n",
      "    analysisdate64 = np.datetime64(analysisdate)\n",
      "    todaynow64 = np.datetime64(datetime.utcnow())\n",
      "    \n",
      "    print \"Today is \", np.datetime_as_string(todaynow64)[0:10]\n",
      "    print \"QueryResults have timestamp of \", np.datetime_as_string(analysisdate64)[0:10]\n",
      "    print \"Creating new time measurements from datestamps ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    df_Competitions['Day0'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_Competitions['Day3'] = df_Competitions.Day0 + 3\n",
      "    df_Competitions['Day7'] = df_Competitions.Day0 + 7\n",
      "    df_Competitions['Day15'] = df_Competitions.Day0 + 15\n",
      "    df_Competitions['Day30'] = df_Competitions.Day0 + 30\n",
      "    \n",
      "    df_Competitions['DayEnd'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    \n",
      "    df_Competitions['InstructorPrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X6 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['PrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationInt'] = (df_Competitions.DayEnd - df_Competitions.Day0)\n",
      "    \n",
      "    df_Competitions['DayOfYear'] = pd.to_datetime(df_Competitions.X7.values).dayofyear\n",
      "    df_Competitions['DayOfWeek'] = pd.to_datetime(df_Competitions.X7.values).dayofweek + 1   # Monday = 1, Sunday = 7\n",
      "    \n",
      "    df_Competitions['Postprocess_sec'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X19 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 1000000000\n",
      "    \n",
      "    df_Competitions['AgeInDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['RemainingDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    # Delete all rows where comp is too young to analyze\n",
      "    #   To use here, a comp needs to be >30 days old (Today - X7 > 30d)\n",
      "    #   and need to be < 30 days out to the final Deadline (Today - X8 > -30)\n",
      "    \n",
      "    print \"Dropping competitions that are too young to analyze ...\"\n",
      "    df_Competitions = df_Competitions[ ((df_Competitions.AgeInDays > 30) & (df_Competitions.RemainingDays > -30))]\n",
      "    \n",
      "    # Drop rows for strange data (these dont seem to be real comps)\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 2553]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 2554]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 3321]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 3867]\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 3894]\n",
      "    # Drop rows for comps less than 1.5 days duration (note: hackathons will be dropped)\n",
      "    df_Competitions = df_Competitions[df_Competitions.DurationDays > 1.5]\n",
      "    \n",
      "    unique_comps = np.unique(df_Competitions.index.values)\n",
      "    print\n",
      "    print \"%d competitions are remaining for analysis\" % len(unique_comps)\n",
      "\n",
      "    # Create future JOIN table of foreign keys\n",
      "    print \"Saving future JOIN tables ...\"\n",
      "    df_ForeignKeys = df_Competitions[['X10','X12','X13','X16','X17','X18','X22','X23','Day0','DayEnd']]\n",
      "    try:\n",
      "        df_ForeignKeys.to_csv('./data/cleaned/foreignkeys.csv')\n",
      "        df_Competitions[['X1','X2','X3','X7']].to_csv('./data/raw/comps_names.csv')\n",
      "        print \"=  Clean output: /data/cleaned/foreignkeys.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/foreignkeys.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        \n",
      "    # Drop unneeded columns\n",
      "    df_Competitions = df_Competitions.drop(['X10','X12','X13','X16','X17','X18','X22','X23'], axis=1)   # the foreign key columns\n",
      "    df_Competitions = df_Competitions.drop(['X2','X3','X4'], axis=1)                                    # the descriptive text\n",
      "    df_Competitions = df_Competitions.drop(['X19'], axis=1)                        # Unneeded date column (now measured in Postprocess_sec)\n",
      "    df_Competitions = df_Competitions.drop(['AgeInDays','RemainingDays'], axis=1)  # Temp columns were needed only to filter out one-time\n",
      "    \n",
      "    \n",
      "    # Visualizations for exploration\n",
      "    def visualizeme(df):\n",
      "        \n",
      "        t0 = time()\n",
      "        \n",
      "        # Instantiate figure0 for histograms\n",
      "        fig = plt.figure(num=0, figsize=(15,22))\n",
      "        fig.suptitle('Histograms of new features', fontsize=14)\n",
      "        ax1 = fig.add_subplot(4,1,1)\n",
      "        ax2 = fig.add_subplot(4,1,2)\n",
      "        ax3 = fig.add_subplot(4,1,3)\n",
      "        ax4 = fig.add_subplot(4,1,4)\n",
      "        \n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"count\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #hist 1\n",
      "            range_of_1_std = df.DurationInt.std() + df.DurationInt.mean()\n",
      "            ax1.hist(df.DurationInt.values, bins=15, range=(0,range_of_1_std), color='r',alpha=0.5)\n",
      "            ax1.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "            #hist 2\n",
      "            range_of_1_std = df.PrepDays.std() + df.PrepDays.mean()\n",
      "            ax2.hist(df.PrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.5)\n",
      "            ax2.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #hist 3\n",
      "            range_of_1_std = df.InstructorPrepDays.std() + df.InstructorPrepDays.mean()\n",
      "            ax3.hist(df.InstructorPrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.25)\n",
      "            ax3.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #hist 4\n",
      "            range_of_1_std = df.Postprocess_sec.std() + df.Postprocess_sec.mean()\n",
      "            ax4.hist(df.Postprocess_sec.values, bins=25, range=(0,range_of_1_std), color='g',alpha=0.8)\n",
      "            ax4.set_xlabel(\"(sec)\", fontsize=10)\n",
      "            ax4.set_title('private leaderboard Postprocess_sec', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting histograms\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_histograms.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_histograms.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_histograms.png  \u00af\\_(\u30c4)_/\u00af\"\n",
      "            print\n",
      "            return\n",
      "        \n",
      "        t0 = time()\n",
      "        # Instantiate figure 1 for boxplots\n",
      "        fig = plt.figure(num=1, figsize=(15,8))\n",
      "        fig.suptitle('Outliers in Preparation days', fontsize=14)\n",
      "        ax1 = fig.add_subplot(1,3,1)\n",
      "        ax2 = fig.add_subplot(1,3,2)\n",
      "        ax3 = fig.add_subplot(1,3,3)\n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"days\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #boxplot 1\n",
      "            ax2.boxplot(df.PrepDays.values, sym='go')\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #boxplot 2\n",
      "            ax3.boxplot(df.InstructorPrepDays.values, sym='bo')\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #boxplot 3\n",
      "            ax1.boxplot(df.DurationInt.values, sym='ro')\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting boxplots\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_outliers.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_outliers.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_outliers.png  \u00af\\_(\u30c4)_/\u00af\"\n",
      "            print\n",
      "            return\n",
      "    \n",
      "    if visualize == True:\n",
      "        visualizeme(df_Competitions)\n",
      "    \n",
      "    # Drop remaining columns no longer needed\n",
      "    df_Competitions = df_Competitions.drop(['X1','X5','X6','X7','X8','X9','X28','X29'], axis=1)\n",
      "    # TODO: more to come? ... \n",
      "    \n",
      "    # Write to /data/cleaned folder\n",
      "    try:\n",
      "        df_Competitions.to_csv('./data/cleaned/comps.csv')\n",
      "        print \"=  Clean output: /data/cleaned/comps.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/comps.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Reclaim memory\n",
      "    df_Competitions = []\n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return coded_legend, unique_comps\n",
      "    \n",
      "    \n",
      "# Load RuleAccepters (1790)\n",
      "def clean1790(filename1, filename2):\n",
      "    \"\"\" ingests, cleans, and munges columns from\n",
      "      /raw/QueryResults1790.csv (User activity log of click acceptance)\n",
      "      /raw/QueryResults1833.csv (User activity log of data downloads, before 2011-04-01)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/rules_accepters.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_RuleAccepters = pd.read_csv(filename1,header=0)\n",
      "        df_Downloaders = pd.read_csv(filename2,header=0,usecols=[1,2,4,5])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1790.csv and QueryResults1833.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_RuleAccepters['LogDate'] = df_RuleAccepters.LogDate.map(lambda x:x[0:10])  #dumb down to just day\n",
      "    df_Downloaders['LogDate'] = df_Downloaders.LogDate.map(lambda x:x[0:10]) \n",
      "    df_RuleAccepters['LogDate'] = pd.to_datetime(df_RuleAccepters.LogDate, utc=True)\n",
      "    df_Downloaders['LogDate'] = pd.to_datetime(df_Downloaders.LogDate, utc=True)\n",
      "\n",
      "    # Note: Formal Rules acceptance was introduced on 2011-04-01, before then there is no log data;\n",
      "    #  so <2011-04-01 I must make an assumption that 'joining' is day of user's first download of any data file\n",
      "    print \"Assuming the implied rules acceptance for early 2010-2011, before recorded logs (\u2299\u2026\u2299 ) ...\"\n",
      "\n",
      "    # Regex find the competitionId in the file url\n",
      "    df_Downloaders['CompetitionId'] = df_Downloaders.BlobUrl\n",
      "    df_Downloaders['CompetitionId'] = df_Downloaders['CompetitionId'].map(lambda x: re.match( r'(.*competitions-data/[A-Za-z]*/)([0-9]*).*', x).group(2)).astype(int)\n",
      "    df_Downloaders.drop(['DownloadedBlobFileId','BlobUrl'],axis=1,inplace=True)\n",
      "    # sort for first day of user, then drop duplicates\n",
      "    df_Downloaders.sort(['CompetitionId','UserId','LogDate'], ascending=[1, 1, 1], inplace=True, axis=0)\n",
      "    df_Downloaders.drop_duplicates(cols=['CompetitionId','UserId'], take_last=False, inplace=True)\n",
      "    \n",
      "    # Get counts using function .size()\n",
      "    by_comp_by_day = df_Downloaders.groupby(['CompetitionId','LogDate'])\n",
      "    df_InferredRulesAccepters = by_comp_by_day.size().to_frame()\n",
      "    # Munge the dataframe to be constructed like QueryResults1790\n",
      "    df_InferredRulesAccepters.reset_index(inplace=True)\n",
      "    df_InferredRulesAccepters.rename(columns={0:'RuleAccepted'}, inplace=True)\n",
      "\n",
      "    # Append to existing Submissions\n",
      "    df_RuleAccepters = pd.concat([df_RuleAccepters, df_InferredRulesAccepters], axis=0)\n",
      "\n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_RuleAccepters['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_RuleAccepters.LogDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "\n",
      "    # QueryResults has null CompetitionIds -> when user accepted sitewide Terms of Service\n",
      "    # remove null CompetitionIds\n",
      "    df_RuleAccepters = df_RuleAccepters[ df_RuleAccepters.CompetitionId.isnull() == False ]\n",
      "    df_RuleAccepters.sort(['CompetitionId','LogDate'], ascending=[1, 1], inplace=True, axis=0)\n",
      "\n",
      "\n",
      "    print \"Contains data for %d competitions,\" % len(np.unique(df_RuleAccepters.CompetitionId))\n",
      "    print \"   constraining to %d competitions under analysis\" % len(master_compids)\n",
      "    print \"   Calculating cumulative total of Accepters per day ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in master_compids:\n",
      "        operate = df_RuleAccepters.iloc[:,[1,3,2]][ (df_RuleAccepters.CompetitionId == c) ].set_index('DaySeq')\n",
      "        cumul = 0\n",
      "        for i in operate.index.values:\n",
      "            cumul = cumul + operate.xs(i,axis=0,copy=False)['RuleAccepted']\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    duration = time() - t0\n",
      "    print \"   ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    \n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulAccepted']).to_csv('./data/cleaned/rules_accepters.csv') \n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/rules_accepters.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/rules_accepters.csv   \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([3,])\n",
      "    operate = []\n",
      "    df_RuleAccepters = []\n",
      "    df_InferredRulesAccepters = []\n",
      "    df_Downloaders = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "\n",
      "# Load Forums table (1792)\n",
      "def clean1792(filename):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1792.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/forum_msg.csv\n",
      "      /cleaned/forum_msg_length.csv\"\"\"\n",
      "    \n",
      "    t00 = time()\n",
      "    try:\n",
      "        df_Forums = pd.read_csv(filename,header=0,usecols=[0,1,2,3,4,5,6,7],converters={'Message': lambda x: len(str(x).split()) })\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,4,9,10])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1792.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Join to get CompId\n",
      "    df_Forums = pd.merge(df_keys, df_Forums, left_on='X16', right_on='ForumId', how='inner')\n",
      "    df_Forums.rename(columns={'Message':'MessageLen'}, inplace=True)                \n",
      "    \n",
      "    # Sort and count Forum Messages\n",
      "    print \"Counting the forum messages per day ...\"\n",
      "    t0 = time()\n",
      "    \n",
      "    df_ForumMg = df_Forums.drop(['X16','Name','Name.1','ForumId','ForumTopicId','FlaggedCount'], axis=1)\n",
      "    df_ForumMg['PostDate'] = df_ForumMg.PostDate.map(lambda x:x[0:10])   # dumb down timestamp to just day, keeping it as String\n",
      "\n",
      "    # Now convert PostDate to a datetime\n",
      "    df_ForumMg['PostDate'] = pd.to_datetime(df_ForumMg.PostDate, utc=True)  \n",
      "    df_ForumMg['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_ForumMg.PostDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_ForumMg = df_ForumMg.drop(['PostDate'], axis=1) \n",
      "    \n",
      "    \n",
      "\n",
      "    df_ForumMg.set_index(['Id','ForumMessageId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    df_ForumMg.sort(['Id','ForumMessageId'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    # create a DataFrameGroupBy:\n",
      "    by_comp_by_day = df_ForumMg.groupby(['Id','DaySeq'])\n",
      "    \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_ForumMgCount = by_comp_by_day.size().to_frame().to_csv('./data/cleaned/forum_msg.csv')\n",
      "        df_ForumMgCount = []\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/forum_msg.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/forum_msg.csv   \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Iterative calc on length of Forum Messages\n",
      "    print \"Calculating the average word-length per day (14-day rolling mean) ...\"\n",
      "    print \"   Contains data for %d competitions,\" % len(np.unique(df_ForumMg.Id))\n",
      "    print \"   constraining to %d competitions under analysis\" % len(master_compids)\n",
      "    t0 = time()\n",
      "    \n",
      "    measurem_array = np.empty([3,],dtype=float)\n",
      "    df_ForumMg.sort(['Id','DaySeq'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    \n",
      "    for c in master_compids:\n",
      "        subslice = df_ForumMg[df_ForumMg.Id == c]\n",
      "        if len(subslice) > 0:\n",
      "            minday = np.int(subslice.Day0.min())\n",
      "            maxday = np.int(subslice.DayEnd.max())\n",
      "            for i in range(minday, maxday+1):\n",
      "                get_2week_avg = subslice[(subslice.DaySeq <= i ) & (subslice.DaySeq >= i-14)]['MessageLen'].mean()\n",
      "                if pd.isnull(get_2week_avg) == True:\n",
      "                    get_2week_avg = 0\n",
      "                collectrow = [c, i, get_2week_avg]\n",
      "                measurem_array = np.vstack([measurem_array, collectrow])\n",
      "    collectrow = []\n",
      "    measurem_array = measurem_array[1:]  # delete first row artifact from creating an np.empty  \n",
      "    \n",
      "    try:\n",
      "        pd.DataFrame(data=measurem_array, columns=['CompetitionId','DaySeq','MessageLen2WeekTrailingAvg']).to_csv('./data/cleaned/forum_msg_length.csv') \n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/forum_msg_length.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/forum_msg_length.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "\n",
      "    # TODO: ForumTopics - later if time permits\n",
      "    \n",
      "    ## Sort and count Forum Topics\n",
      "    #t0 = time()\n",
      "    #print \"Counting Forum topics per day...\"\n",
      "    #df_ForumTp = df_Forums.drop(['X16','Name','Name.1','MessageLen','FlaggedCount'], axis=1)\n",
      "    #      #df_ForumTp.set_index(['Id','ForumId','ForumTopicId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    #df_ForumTp['PostDate'] = df_ForumTp.PostDate.map(lambda x:x[0:10])  # dumb down timestamp to just day\n",
      "    #df_ForumTp['PostDate'] = pd.to_datetime(df_ForumTp.PostDate, utc=True)\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_keys = []\n",
      "    df_ForumMgCount = []\n",
      "    df_Forums = []\n",
      "    df_ForumMg = []\n",
      "    by_comp_by_day = []\n",
      "    subslice = []\n",
      "    measurem_array = []\n",
      "\n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Load Submissions tables (1810-1814, 1820)\n",
      "def clean1810(filename1=\"data/raw/QueryResults1810.csv\", filename2=\"data/raw/QueryResults1811.csv\", filename3=\"data/raw/QueryResults1812.csv\", filename4=\"data/raw/QueryResults1813.csv\", filename5=\"data/raw/QueryResults1814.csv\", filename6=\"data/raw/QueryResults1820.csv\"):\n",
      "    \"\"\" \n",
      "     ingests, cleans, and munges columns from /raw/QueryResults1810 - 1814.csv (leaderboard entries) \n",
      "     and QueryResults1820.csv (Prospect entries)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_globally.csv\n",
      "      /cleaned/submissions.csv\"\"\"\n",
      "\n",
      "    try:\n",
      "        df_Submissions = pd.concat([pd.read_csv(filename1, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename2, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename3, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename4, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename5, header=0, index_col=0)],\n",
      "                                   axis=0)\n",
      "        df_Prospector = pd.read_csv(filename6, header=0, usecols=[1,2,3,4,6])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1810 - 1814.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2, filename3, filename4, filename5, filename6\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With valid submissions JOINED to team members, found %d entries\" % len(df_Submissions)\n",
      "    print \"Adding %d Prospector entries as 'submissions' ...\" % len(df_Prospector)\n",
      "    # Ingest prospect entries\n",
      "    # Set SubmId so high it could not collide with any Submissions table Id's\n",
      "    df_Prospector['SubmId']=df_Prospector.SubmId + 10000000\n",
      "    # Create fake TeamIds so they do not groupby to the same Null\n",
      "    df_Prospector['TeamId']=df_Prospector.UserId + df_Prospector.TotalVotes + 10000000\n",
      "    df_Prospector = df_Prospector.drop(['TotalVotes'], axis=1)\n",
      "    df_Prospector.rename(columns={'DateCreated':'DateSubmitted'}, inplace=True) \n",
      "    df_Prospector.set_index(['SubmId'],drop=True,inplace=True,verify_integrity=True)\n",
      "    # Append to existing Submissions\n",
      "    df_Submissions = pd.concat([df_Submissions, df_Prospector], axis=0)\n",
      "\n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_Submissions['DateSubmitted'] = df_Submissions.DateSubmitted.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_Submissions['DateSubmitted'] = pd.to_datetime(df_Submissions.DateSubmitted, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_Submissions['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Submissions.DateSubmitted - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_Submissions = df_Submissions[ df_Submissions.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    # Output 1: Count of valid-submitting Users globally on the site\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_SubmUsersGlobal = df_Submissions.drop(['DateSubmitted','PublicScore','ScoreStatus','TeamId','CompetitionId'], axis=1)\n",
      "    # Set and sort on a new index to guarantee User falls with first submission ever\n",
      "    df_SubmUsersGlobal.set_index([df_SubmUsersGlobal.index,'UserId'],drop=False,inplace=True)\n",
      "    df_SubmUsersGlobal.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    \n",
      "    \n",
      "    print \"Calculate cumulative total of global Submitting Users per day:\"\n",
      "    print \"  Constraining to unique users ...\"\n",
      "    t0 = time()\n",
      "    # Constrain to unique Users, taking first day appears\n",
      "    df_SubmUsersGlobal.drop_duplicates(cols='UserId', take_last=False, inplace=True)\n",
      "    # Now set index to days, over which to iterate\n",
      "    df_SubmUsersGlobal.set_index('DaySeq', drop=True, inplace=True)\n",
      "    \n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([2,],dtype=float)\n",
      "    cumul = 0\n",
      "    for i in np.unique(df_SubmUsersGlobal.index.values):\n",
      "        cumul = cumul + df_SubmUsersGlobal.xs(i,axis=0,copy=False)['UserId'].size\n",
      "        cumul_array = np.vstack([cumul_array, [ i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['DaySeq','CumulUsers']).to_csv('./data/cleaned/users_globally.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/users_globally.csv\"\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_globally.csv\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Output 2: Count of valid submissions per competition\n",
      "    print \"Calculate submissions made to competitions each day:\"\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_Submissions = df_Submissions.drop(['UserId'], axis=1)\n",
      "    df_Submissions.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Reduce to %d unique Submissions ...\" % len(df_Submissions)\n",
      "    \n",
      "    df_Submissions = df_Submissions.drop(['DateSubmitted','ScoreStatus','TeamId'], axis=1)\n",
      "    df_Submissions.sort(['CompetitionId','DaySeq'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    print \"  Contains data for %d competitions, but\" % len(np.unique(df_Submissions.CompetitionId))\n",
      "    print \"  won't attempt to contrain these\"\n",
      "    \n",
      "    by_comp_by_day = df_Submissions.groupby(['CompetitionId','DaySeq'])\n",
      "    \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_SubmissionsCount = by_comp_by_day.size().to_frame().to_csv('./data/cleaned/submissions.csv')\n",
      "        df_SubmissionsCount = []\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/submissions.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/submissions.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_SubmUsersGlobal = []\n",
      "    df_Submissions = []\n",
      "    by_comp_by_day = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "\n",
      "# Load Teams-Users table (1815)\n",
      "def clean1815(filename1=\"data/raw/QueryResults1815.csv\", filename2=\"data/raw/QueryResults1820.csv\"):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1815.csv \n",
      "     and and QueryResults1820.csv (Prospect entries)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/teams_users.csv\n",
      "      /cleaned/teams.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_TeamsUsers = pd.read_csv(filename1,header=0,usecols=[0,1,2,5,6])\n",
      "        df_Prospector = pd.read_csv(filename2, header=0, usecols=[1,2,3,4,6])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1815.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With teams JOINED to team members, found %d entries\" % len(df_TeamsUsers)\n",
      "    \n",
      "    # Use also QueryResults1820.csv -> in order to include Users on Prospect competitions\n",
      "    # constrain to unique userids per compid\n",
      "    df_Prospector.sort(['CompetitionId','UserId','DateCreated'], ascending=[1, 1, 1], inplace=True, axis=0)\n",
      "    df_Prospector.drop_duplicates(cols=['CompetitionId','UserId'], take_last=False, inplace=True)\n",
      "    print \"Also Interpreting %d Prospector entrants as 'team members'...\" % len(df_Prospector)\n",
      "    \n",
      "    # Ingest prospect entries\n",
      "    # Create fake TeamIds so they do not groupby to the same Null\n",
      "    df_Prospector['TeamId'] = df_Prospector.UserId + df_Prospector.TotalVotes + 10000000\n",
      "    df_Prospector = df_Prospector.drop(['TotalVotes','SubmId'], axis=1)\n",
      "    df_Prospector.rename(columns={'DateCreated':'ScoreFirstSubmittedDate'}, inplace=True) \n",
      "    # Append at bottom to existing TeamsUsers\n",
      "    df_TeamsUsers = pd.concat([df_TeamsUsers, df_Prospector], axis=0)\n",
      "\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_TeamsUsers['ScoreFirstSubmittedDate'] = df_TeamsUsers.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_TeamsUsers['ScoreFirstSubmittedDate'] = pd.to_datetime(df_TeamsUsers.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_TeamsUsers['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_TeamsUsers.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_TeamsUsers = df_TeamsUsers.drop(['ScoreFirstSubmittedDate'], axis=1) \n",
      "    \n",
      "    \n",
      "    # Output 1: Count of users on teams\n",
      "    \n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeamsUsers = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    print \"Calculate cumulative total of *users on teams* per day:\"\n",
      "    t0 = time()\n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeamsUsers.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeamsUsers.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeamsUsers.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeamsUsers.DaySeq[df_CumulTeamsUsers.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeamsUsers.xs([c,i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeamsUsers']).to_csv('./data/cleaned/teams_users.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/teams_users.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_users.csv   \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "        \n",
      "    # Output 2: Count of multiplayer teams\n",
      "    \n",
      "    try:\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "    except IOError:\n",
      "        print \" (>_<)  WARNING: Can't find input file: data/cleaned/foreignkeys.csv\"\n",
      "        print\n",
      "        return  \n",
      "        \n",
      "    print \"Count the multiplayer teams who participated:\"\n",
      "    t0 = time()\n",
      "    # Join to get DayEnd\n",
      "    df_TeamsUsers = pd.merge(df_keys, df_TeamsUsers, left_on='Id', right_on='CompetitionId', how='inner')             \n",
      "      \n",
      "    # remove null CompetitionIds\n",
      "    print \"  Enforce that CompetitionId is not null ...\"\n",
      "    df_MultiTeams = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    # Drop unneeded columns\n",
      "    df_MultiTeams = df_MultiTeams.drop(['Ranking'], axis=1)\n",
      "    print \"  Counting members on each team ...\"\n",
      "    df_MultiTeams['Members'] = df_MultiTeams[['TeamId','CompetitionId']].groupby(['TeamId']).transform('count')\n",
      "    print \"  Dropping single player teams ...\"\n",
      "    df_MultiTeams = df_MultiTeams[df_MultiTeams.Members > 1]\n",
      "    df_MultiTeams = df_MultiTeams[df_MultiTeams.DaySeq <= df_MultiTeams.DayEnd]\n",
      "    df_MultiTeams = df_MultiTeams.drop(['Id','DayEnd','UserId'], axis=1)\n",
      "    df_MultiTeams.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Found %d multiplayer teams\" % len(df_MultiTeams)\n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    t0 = time()\n",
      "    by_comp = df_MultiTeams.groupby(['CompetitionId'])\n",
      "        \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_MultiTeamsCount = by_comp.size().to_frame().to_csv('./data/cleaned/teams_multiplayer.csv')\n",
      "        df_MultiTeamsCount = []\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/teams_multiplayer.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "        print\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_multiplayer.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "        \n",
      "        \n",
      "    # Output 3: Count of teams\n",
      "    \n",
      "    print \"Calculate cumulative total of teams per day:\"\n",
      "    t0 = time()\n",
      "    # remove null CompetitionIds\n",
      "    print \"  Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeams = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_CumulTeams = df_CumulTeams.drop(['UserId', 'Ranking'], axis=1)\n",
      "    df_CumulTeams.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Reduce to %d unique teams ...\" % len(df_CumulTeams)\n",
      "    \n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeams.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeams.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    \n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeams.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeams.DaySeq[df_CumulTeams.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeams.xs([c,i],axis=0,copy=False)['TeamId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeams']).to_csv('./data/cleaned/teams.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/teams.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams.csv   \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_TeamsUsers = []\n",
      "    df_MultiTeams = []\n",
      "    df_MultiTeamsCount = []\n",
      "    df_CumulTeamUsers = []\n",
      "    df_CumulTeams = []\n",
      "    df_keys = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Load Teams-Users-Bios table (1816)\n",
      "def clean1816(filename=\"data/raw/QueryResults1816.csv\"):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1816.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/teams_users_bios.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_TeamsUsersBios = pd.read_csv(filename,header=0,usecols=[0,1,2,4])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1816.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With teams JOINED to team members with bios, found %d entries\" % len(df_TeamsUsersBios)\n",
      "    \n",
      "    # TODO: create a new query similar to 1816 to collect Prospect users with bios\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_TeamsUsersBios['ScoreFirstSubmittedDate'] = df_TeamsUsersBios.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_TeamsUsersBios['ScoreFirstSubmittedDate'] = pd.to_datetime(df_TeamsUsersBios.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_TeamsUsersBios['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_TeamsUsersBios.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_TeamsUsersBios = df_TeamsUsersBios.drop(['ScoreFirstSubmittedDate'], axis=1) \n",
      "    \n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeamsUsers = df_TeamsUsersBios[ df_TeamsUsersBios.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    print \"Calculate cumulative total of *users on teams* per day:\"\n",
      "    t0 = time()\n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeamsUsers.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeamsUsers.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeamsUsers.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeamsUsers.DaySeq[df_CumulTeamsUsers.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeamsUsers.xs([c,i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeamsUsersBios']).to_csv('./data/cleaned/teams_users_bios.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/teams_users_bios.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_users_bios.csv   \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_TeamsUsersBios = []\n",
      "    df_CumulTeamUsers = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Load Key Countries table (1808)\n",
      "def clean1808(filename=\"data/raw/QueryResults1808.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1808.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_countries.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1808.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count of users from key countries:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_CountCountries = pd.merge(df_keys, df_CountCountries, left_on='Id', right_on='CompetitionId', how='left')\n",
      "    df_CountCountries['CountUserIdsOnTeams'].fillna(value=0,inplace=True)\n",
      "    df_CountCountries = df_CountCountries.drop(['CompetitionId','DayEnd'], axis=1)\n",
      "    df_CountCountries.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_CountCountries.to_csv('./data/cleaned/users_countries.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/users_countries.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_countries.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Find Universe of users on submitting teams for Key Countries (1560)\n",
      "def clean1560(filename=\"data/raw/QueryResults1560.csv\"):\n",
      "    \"\"\" ingests, and munges columns from /raw/QueryResults1560.csv\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_globally_countries.csv\"\"\"\n",
      "\n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename, header=0)\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1560.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "\n",
      "    t00 = time()\n",
      "    print \"Calculate universe of users on teams for %d key countries:\" % len(np.unique(df_CountCountries.Country.values))\n",
      "    print \"  Geolocation is known for %d submitting teams users\" % len(df_CountCountries)\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"  Force column type of date field ...\"\n",
      "    df_CountCountries['ScoreFirstSubmittedDate'] = df_CountCountries.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_CountCountries['ScoreFirstSubmittedDate'] = pd.to_datetime(df_CountCountries.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_CountCountries['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_CountCountries.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    \n",
      "    print \"  Constraining to unique users ...\"\n",
      "    # Set and sort on a new index to guarantee User falls with first submission ever\n",
      "    df_CountCountries.set_index(['UserId','DaySeq'],drop=False,inplace=True)\n",
      "    df_CountCountries.sort_index(axis=0, ascending=[1,1], inplace=True)\n",
      "    # Constrain to unique Users, taking first day appears\n",
      "    df_CountCountries.drop_duplicates(cols='UserId', take_last=False, inplace=True)\n",
      "    print \"  ::Reduced to %d unique submitting users\" % len(df_CountCountries)\n",
      "    # Now set index to days, over which to iterate\n",
      "    df_CountCountries.set_index(['Country','DaySeq'], drop=False, inplace=True)\n",
      "    df_CountCountries.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    \n",
      "    t0 = time()\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    cumul = 0\n",
      "    for c in np.unique(df_CountCountries.Country.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CountCountries.DaySeq[df_CountCountries.Country == c].values):\n",
      "            cumul = cumul + df_CountCountries.xs([c, i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['Country','DaySeq','CumulUsers']).to_csv('./data/cleaned/users_globally_countries.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/users_globally_countries.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_globally_countries.csv \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return       \n",
      "\n",
      "\n",
      "# Load All Countries table (1809)\n",
      "def clean1809(filename=\"data/raw/QueryResults1809.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1809.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_countries_all.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1809.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count of all countries represented by CreatedIPaddr of users:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_CountCountries = pd.merge(df_keys, df_CountCountries, left_on='Id', right_on='CompetitionId', how='left')\n",
      "    df_CountCountries.fillna(value=0,inplace=True)\n",
      "    df_CountCountries = df_CountCountries.drop(['CompetitionId'], axis=1)\n",
      "    df_CountCountries.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_CountCountries.to_csv('./data/cleaned/users_countries_all.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/users_countries_all.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_countries_all.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Load LimitedParticipation table (1842)\n",
      "def clean1842(filename=\"data/raw/QueryResults1842.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1842.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_limited_entry.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_LimitedGroups = pd.read_csv(filename,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,7,8])\n",
      "        print\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1842.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count maximum users in limited entry competitions:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_keys.X23[pd.isnull(df_keys.X23) == True] = df_keys.X22\n",
      "    df_LimitedGroups = pd.merge(df_LimitedGroups, df_keys, left_on='GroupId', right_on='X23', how='inner')\n",
      "    df_LimitedGroups = df_LimitedGroups.drop(['GroupTypeId','X22','X23'], axis=1)\n",
      "    df_LimitedGroups.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_LimitedGroups.to_csv('./data/cleaned/users_limited_entry.csv')\n",
      "        print\n",
      "        print \"=  Clean output: /data/cleaned/users_limited_entry.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_limited_entry.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        print\n",
      "        return\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_LimitedGroups = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print\n",
      "    print \"File done in %fs\" % duration\n",
      "    return\n",
      "\n",
      "# Okay, you're ready. Get to cleaning!\n",
      "\n",
      "duration = time() - t000\n",
      "print \"Start:\"\n",
      "print \"Numpy version\", np.__version__\n",
      "print \"Pandas version\", pd.__version__\n",
      "print \"Code loaded in %fs\" % duration\n",
      "print\n",
      "\n",
      "codenames, master_compids = clean1784(\"data/raw/QueryResults1784.csv\", visualize=False)   # Clean Competitions table\n",
      "if True:\n",
      "    clean1790(\"data/raw/QueryResults1790.csv\",\"data/raw/QueryResults1833.csv\")    # Clean RulesAccepters\n",
      "    clean1792(\"data/raw/QueryResults1792.csv\")    # Clean Forums\n",
      "    clean1810(\"data/raw/QueryResults1810.csv\",\"data/raw/QueryResults1811.csv\",\"data/raw/QueryResults1812.csv\",\n",
      "              \"data/raw/QueryResults1813.csv\",\"data/raw/QueryResults1814.csv\",\"data/raw/QueryResults1820.csv\")  # Clean Submissions & Prospectors\n",
      "    clean1815(\"data/raw/QueryResults1815.csv\",\"data/raw/QueryResults1820.csv\")    # Clean Teams-Users\n",
      "    clean1816(\"data/raw/QueryResults1816.csv\")    # Clean Teams-Users with Bios\n",
      "    clean1808(\"data/raw/QueryResults1808.csv\")     # Clean T12 countries \n",
      "    clean1560(\"data/raw/QueryResults1560.csv\")     # Clean T12 countries globally\n",
      "    clean1809(\"data/raw/QueryResults1809.csv\")     # Clean countries All\n",
      "    clean1842(\"data/raw/QueryResults1842.csv\")     # Clean Limited partic groups\n",
      "\n",
      "\n",
      "duration = time() - t000\n",
      "print\n",
      "print \"==============================================}\"\n",
      "print \"DATA_CLEANER COMPLETE! Finished in %fs  }  d=(\u00b4\u25bd\uff40)=b\" % duration\n",
      "print \"==============================================\"\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Start:\n",
        "Numpy version 1.8.0\n",
        "Pandas version 0.13.1\n",
        "Code loaded in 0.022643s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1784.csv\n",
        "________________________________________________________________________________\n",
        "Masking feature names from raw data ...\n",
        "Flatten feature X17 to values {1,3} ...\n",
        "Force column type of boolean and date fields ...\n",
        "Checking for valid dates:\n",
        "   filling 54 missing dates  (\u2299\u2026\u2299 ) ...\n",
        "   found 5 questionable dates relative to Launch  (\u2299\u2026\u2299 ) ...\n",
        "   found 8 more questionable dates. Making assumptions  (\u25ce_\u25ce;)  ...\n",
        "   ::done in 0.027024s\n",
        "Today is  2014-05-17\n",
        "QueryResults have timestamp of  2014-05-09\n",
        "Creating new time measurements from datestamps ...\n",
        "  ::done in 0.031040s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Dropping competitions that are too young to analyze ...\n",
        "\n",
        "366 competitions are remaining for analysis\n",
        "Saving future JOIN tables ...\n",
        "=  Clean output: /data/cleaned/foreignkeys.csv\n",
        "\n",
        "=  Clean output: /data/cleaned/comps.csv\n",
        "\n",
        "File done in 0.115879s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1790.csv and QueryResults1833.csv\n",
        "________________________________________________________________________________\n",
        "Assuming the implied rules acceptance for early 2010-2011, before recorded logs (\u2299\u2026\u2299 ) ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Contains data for 448 competitions,"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   constraining to 366 competitions under analysis\n",
        "   Calculating cumulative total of Accepters per day ...\n",
        "   ::done in 16.649789s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output: /data/cleaned/rules_accepters.csv\n",
        "\n",
        "File done in 17.677403s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1792.csv\n",
        "________________________________________________________________________________\n",
        "Counting the forum messages per day ...\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output: /data/cleaned/forum_msg.csv\n",
        "   ::done in 0.482296s\n",
        "Calculating the average word-length per day (14-day rolling mean) ...\n",
        "   Contains data for 200 competitions,\n",
        "   constraining to 366 competitions under analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output: /data/cleaned/forum_msg_length.csv\n",
        "   ::done in 23.756991s\n",
        "\n",
        "File done in 25.006481s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1810 - 1814.csv\n",
        "________________________________________________________________________________\n",
        "With valid submissions JOINED to team members, found 650053 entries\n",
        "Adding 596 Prospector entries as 'submissions' ...\n",
        "Force column type of date fields ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of global Submitting Users per day:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Constraining to unique users ...\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 2.669074s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally.csv\n",
        "\n",
        "Calculate submissions made to competitions each day:\n",
        "  Reduce to 449268 unique Submissions ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Contains data for 439 competitions, but"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  won't attempt to contrain these\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output: /data/cleaned/submissions.csv\n",
        "   ::done in 8.378719s\n",
        "\n",
        "File done in 17.575364s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1815.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members, found 47021 entries\n",
        "Also Interpreting 509 Prospector entrants as 'team members'...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 9.498143s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users.csv\n",
        "Count the multiplayer teams who participated:\n",
        "  Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Counting members on each team ...\n",
        "  Dropping single player teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Found 3325 multiplayer teams\n",
        "  ::done in 26.020288s\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_multiplayer.csv\n",
        "   ::done in 0.009041s\n",
        "\n",
        "Calculate cumulative total of teams per day:\n",
        "  Enforce that CompetitionId is not null ...\n",
        "  Reduce to 35173 unique teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 7.349555s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams.csv\n",
        "\n",
        "File done in 43.505099s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1816.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members with bios, found 10014 entries\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ...\n",
        "  ::done in 3.585702s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users_bios.csv\n",
        "\n",
        "File done in 3.733772s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1808.csv\n",
        "________________________________________________________________________________\n",
        "Count of users from key countries:\n",
        "   constraining to 366 competitions under analysis ...\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=  Clean output: /data/cleaned/users_countries.csv\n",
        "\n",
        "File done in 0.027844s\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "cleaner for QueryResults1560.csv\n",
        "________________________________________________________________________________\n",
        "Calculate universe of users on teams for 12 key countries:\n",
        "  Geolocation is known for 36038 submitting teams users\n",
        "  Force column type of date field ...\n",
        "  Constraining to unique users ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::Reduced to 21596 unique submitting users"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ...\n",
        "  ::done in 6.909071s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally_countries.csv\n",
        "\n",
        "File done in 7.659394s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1809.csv\n",
        "________________________________________________________________________________\n",
        "Count of all countries represented by CreatedIPaddr of users:\n",
        "   constraining to 366 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_countries_all.csv\n",
        "\n",
        "File done in 0.009029s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1842.csv\n",
        "________________________________________________________________________________\n",
        "Count maximum users in limited entry competitions:\n",
        "   constraining to 366 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_limited_entry.csv\n",
        "\n",
        "File done in 0.006091s\n",
        "\n",
        "==============================================}\n",
        "DATA_CLEANER COMPLETE! Finished in 117.413137s  }  d=(\u00b4\u25bd\uff40)=b\n",
        "==============================================\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}