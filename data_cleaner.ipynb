{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Class data_cleaner"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: RJ Ramey <rj.github@garimeme.com>\n",
      "# License: (c) 2014 by RJ Ramey. All rights reserved. \n",
      "# No license is given at this time.\n",
      "\n",
      "# Setup and Initialization\n",
      "from datetime import datetime\n",
      "from time import time\n",
      "import os.path\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
      "\n",
      "t000 = time()\n",
      "origindate = datetime(2010, 1, 1)\n",
      "\n",
      "# IO / Load data functions\n",
      "def modification_date(filename):\n",
      "    \"\"\" return the modified date from filename on disk\"\"\"\n",
      "    t = os.path.getmtime(filename)\n",
      "    return datetime.fromtimestamp(t)\n",
      "\n",
      "def printhelp():\n",
      "    \"\"\" prints expected inputs to the console \"\"\"\n",
      "    \n",
      "    print \"\u300c(\u00b0\u30d8\u00b0)  Oops: data_cleaner expects certain (non-public) files in /data/raw/ :\"\n",
      "    print \"\\n  Prerequisite files: \"\n",
      "    print \"  - QueryResults1560.csv  from https://sql.kaggle.com/kaggle/query/1560/\"\n",
      "    print \"  - QueryResults1784.csv  from https://sql.kaggle.com/kaggle/query/1784/\"\n",
      "    print \"  - QueryResults1790.csv  from https://sql.kaggle.com/kaggle/query/1790/\"\n",
      "    print \"  - QueryResults1792.csv  from https://sql.kaggle.com/kaggle/query/1792/\"\n",
      "    print \"  - QueryResults1807.csv  from https://sql.kaggle.com/kaggle/query/1807/\"\n",
      "    print \"  - QueryResults1808.csv  from https://sql.kaggle.com/kaggle/query/1808/\"\n",
      "    print \"  - QueryResults1809.csv  from https://sql.kaggle.com/kaggle/query/1809/\"\n",
      "    print \"  - QueryResults1810.csv  from https://sql.kaggle.com/kaggle/query/1810/\"\n",
      "    print \"  - QueryResults1811.csv  from https://sql.kaggle.com/kaggle/query/1811/\"\n",
      "    print \"  - QueryResults1812.csv  from https://sql.kaggle.com/kaggle/query/1812/\"\n",
      "    print \"  - QueryResults1813.csv  from https://sql.kaggle.com/kaggle/query/1813/\"\n",
      "    print \"  - QueryResults1814.csv  from https://sql.kaggle.com/kaggle/query/1814/\"\n",
      "    print \"  - QueryResults1815.csv  from https://sql.kaggle.com/kaggle/query/1815/\"\n",
      "    print \"  - QueryResults1816.csv  from https://sql.kaggle.com/kaggle/query/1816/\"\n",
      "    print \"  - QueryResults1820.csv  from https://sql.kaggle.com/kaggle/query/1820/\"\n",
      "    print \"  - QueryResults1833.csv  from https://sql.kaggle.com/kaggle/query/1833/\"\n",
      "    print \"  - QueryResults1842.csv  from https://sql.kaggle.com/kaggle/query/1842/\"\n",
      "    print \"  - QueryResults1855.csv  from https://sql.kaggle.com/kaggle/query/1855/\"\n",
      "    print\n",
      "    return\n",
      "    \n",
      "# Load Competitions table (1784)\n",
      "def clean1784(filename,visualize=True):\n",
      "    \"\"\" ingests, cleans and munges columns from /raw/QueryResults1784.csv \n",
      "    \n",
      "    Returns:\n",
      "      coded_legend: dict of X1,X2,...,X23 column names to their original name from the raw query results\n",
      "      \n",
      "      unique_comps: array of CompetitionId's contained in comps.csv\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/comps.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_Competitions = pd.read_csv(filename,header=0,index_col=0)\n",
      "        print \"================================================================================\"\n",
      "        print \"cleaner for QueryResults1784.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "        \n",
      "    \n",
      "    # Code the feature names for public storage on Github\n",
      "    print \"Masking feature names from raw data ...\"\n",
      "    t00 = time()\n",
      "    coded_names = [ 'X' + str(i) for i in range (1, len(df_Competitions.columns) + 1)]\n",
      "            # save the dictionary for future reversal, will return this value from function\n",
      "    coded_legend = { code: orig for code, orig in (zip(coded_names, df_Competitions.columns))}\n",
      "    \n",
      "    df_Competitions.columns = coded_names\n",
      "    \n",
      "    \n",
      "    # Data Munging\n",
      "    \n",
      "    # Convert X17 {3: 3, all others : 1}\n",
      "    print \"Flatten feature X17 to values {1 or 3} ...\"\n",
      "    df_Competitions.loc[ df_Competitions.X17 <> 3, 'X17' ] = 1 \n",
      "    \n",
      "    print \"Force column type of boolean and date fields ...\"\n",
      "      # Convert X23 to bool, {null: 0, notnull: 1}\n",
      "    df_Competitions['Is_X23'] = df_Competitions.X23.map(lambda x: x/x)     # this evaluates to 1\n",
      "    df_Competitions.loc[ df_Competitions.Is_X23.isnull(), 'Is_X23' ] = 0   # sets remaining 0\n",
      "    coded_legend['Is_X23'] = str('Is' + coded_legend['X23'][0:20])         # save meaning of IsX23 in coded names\n",
      "    \n",
      "    # Convert boolean columns to 1/0\n",
      "    for cn in list(df_Competitions.columns[df_Competitions.dtypes.map(lambda x: x=='bool')]):\n",
      "        df_Competitions[cn] = df_Competitions[cn].astype(int)\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_Competitions['X5'] = pd.to_datetime(df_Competitions.X5, utc=True)   \n",
      "    df_Competitions['X6'] = pd.to_datetime(df_Competitions.X6, utc=True)   \n",
      "    df_Competitions['X7'] = pd.to_datetime(df_Competitions.X7, utc=True)   \n",
      "    df_Competitions['X8'] = pd.to_datetime(df_Competitions.X8, utc=True)   \n",
      "    df_Competitions['X19'] = pd.to_datetime(df_Competitions.X19, utc=True)    \n",
      "    \n",
      "    print \"Checking for valid dates:\"\n",
      "    t0 = time()\n",
      "    \n",
      "    # Check where X5 is missing; assume it to = X6\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X5.isnull() == True])\n",
      "    if flags > 0:\n",
      "        print \"   filling %d missing dates  (\u2299\u2026\u2299 ) ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X5.isnull() == True), 'X5' ] = df_Competitions.X6\n",
      "    \n",
      "    # Check where dates should be previous in time to the subsequent one; force it so\n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ df_Competitions.X6 > df_Competitions.X7])\n",
      "    if flags > 0:\n",
      "        print \"   found %d questionable dates relative to Launch  (\u2299\u2026\u2299 ) ...\" % flags\n",
      "        df_Competitions.loc[ (df_Competitions.X6 > df_Competitions.X7), 'X6' ] = df_Competitions.X7\n",
      "        \n",
      "    flags = 0\n",
      "    flags = len(df_Competitions[ (df_Competitions.X6.isnull() == False) & (df_Competitions.X5 > df_Competitions.X6)])\n",
      "    if flags > 0:\n",
      "        print \"   found %d more questionable dates. Making assumptions  (\u25ce_\u25ce;)  ...\" % flags\n",
      "        \n",
      "        # calculate to find value of X5 that are off by almost exactly 1 year\n",
      "        df_Competitions['X5_X6'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X5 - df_Competitions.X6),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "        \n",
      "        # if seems close to 1 year off, assume human error and subtract 1 year\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 300) & (df_Competitions.X5_X6 < 367) , 'X5' ] = pd.to_datetime(df_Competitions.X5) - np.timedelta64(366,'D')\n",
      "            \n",
      "        # but if not almost exactly 1 year, then just force to same as X6\n",
      "        if len(df_Competitions[df_Competitions.X5_X6 >= 367]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 >= 367), 'X5' ] = df_Competitions.X6\n",
      "        if len(df_Competitions[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300) ]) > 0:\n",
      "            df_Competitions.loc[ (df_Competitions.X5_X6 > 0) & (df_Competitions.X5_X6 <= 300), 'X5' ] = df_Competitions.X6\n",
      "        \n",
      "        # drop the temporary calculation column\n",
      "        df_Competitions = df_Competitions.drop(['X5_X6'], axis=1)\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"   ::done in %fs\" % duration\n",
      "    \n",
      "    \n",
      "    # Additional feature engineering\n",
      "    # do date arithmetic, converts to ns, calculate back to days\n",
      "    analysisdate = modification_date(filename)\n",
      "    analysisdate64 = np.datetime64(analysisdate)\n",
      "    todaynow64 = np.datetime64(datetime.utcnow())\n",
      "    \n",
      "    print \"Today is \", np.datetime_as_string(todaynow64)[0:10]\n",
      "    print \"QueryResults have timestamp of \", np.datetime_as_string(analysisdate64)[0:10]\n",
      "    print \"Creating new time measurements from datestamps ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    df_Competitions['Day0'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_Competitions['Day3'] = df_Competitions.Day0 + 3\n",
      "    df_Competitions['Day7'] = df_Competitions.Day0 + 7\n",
      "    df_Competitions['Day15'] = df_Competitions.Day0 + 15\n",
      "    df_Competitions['Day30'] = df_Competitions.Day0 + 30\n",
      "    \n",
      "    df_Competitions['DayEnd'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    \n",
      "    df_Competitions['InstructorPrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X6 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['PrepDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X7 - df_Competitions.X5),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationDays'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X8 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['DurationInt'] = (df_Competitions.DayEnd - df_Competitions.Day0)\n",
      "    \n",
      "    df_Competitions['DayOfYear'] = pd.to_datetime(df_Competitions.X7.values).dayofyear\n",
      "    df_Competitions['DayOfWeek'] = pd.to_datetime(df_Competitions.X7.values).dayofweek + 1   # Monday = 1, Sunday = 7\n",
      "    \n",
      "    df_Competitions['Postprocess_sec'] = pd.to_timedelta( \n",
      "                                                  (df_Competitions.X19 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 1000000000\n",
      "    \n",
      "    df_Competitions['AgeInDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X7),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    \n",
      "    df_Competitions['RemainingDays'] = pd.to_timedelta( \n",
      "                                                  (analysisdate64 - df_Competitions.X8),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000\n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    # Delete all rows where comp is too young to analyze\n",
      "    #   To use here, a comp needs to be >30 days old (Today - X7 > 30d)\n",
      "    #   and need to be < 30 days out to the final Deadline (Today - X8 > -30)\n",
      "    \n",
      "    print \"Dropping competitions that are too young to analyze ...\"\n",
      "    df_Competitions = df_Competitions[ ((df_Competitions.AgeInDays > 30) & (df_Competitions.RemainingDays > -30))]\n",
      "    \n",
      "    # Drop rows for strange data (these dont seem to be real comps)\n",
      "    for n in (2553,2554,3321,3867,3894):\n",
      "        df_Competitions = df_Competitions[df_Competitions.index <> n]\n",
      "    # Drop rows for comps less than 1.5 days duration (note: hackathons will be dropped)\n",
      "    df_Competitions = df_Competitions[df_Competitions.DurationDays > 1.5]\n",
      "    # Drop first ever competition (low numbers at dawn of time creates massive outliers)\n",
      "    df_Competitions = df_Competitions[df_Competitions.index <> 2408]\n",
      "    \n",
      "    unique_comps = np.unique(df_Competitions.index.values)\n",
      "    print \"\\n%d competitions are remaining for analysis\" % len(unique_comps)\n",
      "\n",
      "    # Create future JOIN table of foreign keys\n",
      "    print \"Saving future JOIN tables ...\"\n",
      "    df_ForeignKeys = df_Competitions[['X10','X12','X13','X16','X17','X18','X22','X23','Day0','DayEnd']]\n",
      "    try:\n",
      "        df_ForeignKeys.to_csv('./data/cleaned/foreignkeys.csv')\n",
      "        df_Competitions[['X1','X2','X3','X7','X12','X17']].to_csv('./data/raw/comps_labels.csv')\n",
      "        print \"=  Clean output: /data/cleaned/foreignkeys.csv \\n\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/foreignkeys.csv  \u00af\\_(\u30c4)_/\u00af\"\n",
      "        return False\n",
      "        \n",
      "    # Drop unneeded columns\n",
      "    df_Competitions = df_Competitions.drop(['X10','X12','X13','X16','X17','X18','X22','X23'], axis=1)   # the foreign key columns\n",
      "    df_Competitions = df_Competitions.drop(['X2','X3','X4'], axis=1)                                    # the descriptive text\n",
      "    df_Competitions = df_Competitions.drop(['X19'], axis=1)                        # Unneeded date column (now measured in Postprocess_sec)\n",
      "    df_Competitions = df_Competitions.drop(['AgeInDays','RemainingDays'], axis=1)  # Temp columns were needed only to filter out one-time\n",
      "    \n",
      "    \n",
      "    # Visualizations for exploration\n",
      "    def visualizeme(df):\n",
      "        \n",
      "        t0 = time()\n",
      "        \n",
      "        # Instantiate figure0 for histograms\n",
      "        fig = plt.figure(num=0, figsize=(15,22))\n",
      "        fig.suptitle('Histograms of new features', fontsize=14)\n",
      "        ax1 = fig.add_subplot(4,1,1)\n",
      "        ax2 = fig.add_subplot(4,1,2)\n",
      "        ax3 = fig.add_subplot(4,1,3)\n",
      "        ax4 = fig.add_subplot(4,1,4)\n",
      "        \n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"count\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #hist 1\n",
      "            range_of_1_std = df.DurationInt.std() + df.DurationInt.mean()\n",
      "            ax1.hist(df.DurationInt.values, bins=15, range=(0,range_of_1_std), color='r',alpha=0.5)\n",
      "            ax1.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "            #hist 2\n",
      "            range_of_1_std = df.PrepDays.std() + df.PrepDays.mean()\n",
      "            ax2.hist(df.PrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.5)\n",
      "            ax2.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #hist 3\n",
      "            range_of_1_std = df.InstructorPrepDays.std() + df.InstructorPrepDays.mean()\n",
      "            ax3.hist(df.InstructorPrepDays.values, bins=25, range=(0,range_of_1_std), color='b',alpha=0.25)\n",
      "            ax3.set_xlabel(\"(days)\", fontsize=10)\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #hist 4\n",
      "            range_of_1_std = df.Postprocess_sec.std() + df.Postprocess_sec.mean()\n",
      "            ax4.hist(df.Postprocess_sec.values, bins=25, range=(0,range_of_1_std), color='g',alpha=0.8)\n",
      "            ax4.set_xlabel(\"(sec)\", fontsize=10)\n",
      "            ax4.set_title('private leaderboard Postprocess_sec', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting histograms\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_histograms.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_histograms.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_histograms.png  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "            return False\n",
      "        \n",
      "        t0 = time()\n",
      "        # Instantiate figure 1 for boxplots\n",
      "        fig = plt.figure(num=1, figsize=(15,8))\n",
      "        fig.suptitle('Outliers in Preparation days', fontsize=14)\n",
      "        ax1 = fig.add_subplot(1,3,1)\n",
      "        ax2 = fig.add_subplot(1,3,2)\n",
      "        ax3 = fig.add_subplot(1,3,3)\n",
      "        all_axes = plt.gcf().axes\n",
      "        for ax in all_axes:\n",
      "            ax.set_ylabel(\"days\", fontsize=10)\n",
      "            for ticklabel in ax.get_xticklabels() + ax.get_yticklabels():\n",
      "                ticklabel.set_fontsize(10)\n",
      "        try:\n",
      "            #boxplot 1\n",
      "            ax2.boxplot(df.PrepDays.values, sym='go')\n",
      "            ax2.set_title('PrepDays [launched - created]', fontsize=12)\n",
      "            #boxplot 2\n",
      "            ax3.boxplot(df.InstructorPrepDays.values, sym='bo')\n",
      "            ax3.set_title('InstructorPrepDays [submitted - created]', fontsize=12)\n",
      "            #boxplot 3\n",
      "            ax1.boxplot(df.DurationInt.values, sym='ro')\n",
      "            ax1.set_title('DurationInt [deadline - launched]', fontsize=12)\n",
      "        except:\n",
      "            print \"An error occurred in plotting boxplots\" \n",
      "            \n",
      "        # Save the figure as one file\n",
      "        try:\n",
      "            plt.savefig('./data/vis/features1_outliers.png')\n",
      "            duration = time() - t0\n",
      "            print \"=  Vis Output: /data/vis/features1_outliers.png\"\n",
      "            print \"   ::done in %fs\" % duration\n",
      "        except IOError:\n",
      "            print \"WARNING: Failed to write out file: data/vis/features1_outliers.png  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "            return False\n",
      "    \n",
      "    if visualize == True:\n",
      "        visualizeme(df_Competitions)\n",
      "    \n",
      "    # Drop remaining columns no longer needed\n",
      "    df_Competitions = df_Competitions.drop(['X1','X5','X6','X7','X8','X9','X28','X29'], axis=1)\n",
      "    # TODO: more to come? ... \n",
      "    \n",
      "    # Write to /data/cleaned folder\n",
      "    try:\n",
      "        df_Competitions.to_csv('./data/cleaned/comps.csv')\n",
      "        print \"=  Clean output: /data/cleaned/comps.csv \\n\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/comps.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Reclaim memory\n",
      "    df_Competitions = []\n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return coded_legend, unique_comps\n",
      "    \n",
      "    \n",
      "# Load RuleAccepters (1790)\n",
      "def clean1790(filename1, filename2):\n",
      "    \"\"\" ingests, cleans, and munges columns from\n",
      "      /raw/QueryResults1790.csv (User activity log of click acceptance)\n",
      "      /raw/QueryResults1833.csv (User activity log of data downloads, before 2011-04-01)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/rules_accepters.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_RuleAccepters = pd.read_csv(filename1,header=0)\n",
      "        df_Downloaders = pd.read_csv(filename2,header=0,usecols=[1,2,4,5])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1790.csv and QueryResults1833.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    df_RuleAccepters['LogDate'] = df_RuleAccepters.LogDate.map(lambda x:x[0:10])  #dumb down to just day\n",
      "    df_Downloaders['LogDate'] = df_Downloaders.LogDate.map(lambda x:x[0:10]) \n",
      "    df_RuleAccepters['LogDate'] = pd.to_datetime(df_RuleAccepters.LogDate, utc=True)\n",
      "    df_Downloaders['LogDate'] = pd.to_datetime(df_Downloaders.LogDate, utc=True)\n",
      "\n",
      "    # Note: Formal Rules acceptance was introduced on 2011-04-01, before then there is no log data;\n",
      "    #  so <2011-04-01 I must make an assumption that 'joining' is day of user's first download of any data file\n",
      "    print \"Assuming the implied rules acceptance for early 2010-2011, before recorded logs (\u2299\u2026\u2299 ) ...\"\n",
      "\n",
      "    # Regex find the competitionId in the file url\n",
      "    df_Downloaders['CompetitionId'] = df_Downloaders.BlobUrl\n",
      "    df_Downloaders['CompetitionId'] = df_Downloaders['CompetitionId'].map(lambda x: re.match( r'(.*competitions-data/[A-Za-z]*/)([0-9]*).*', x).group(2)).astype(int)\n",
      "    df_Downloaders.drop(['DownloadedBlobFileId','BlobUrl'],axis=1,inplace=True)\n",
      "    # sort for first day of user, then drop duplicates\n",
      "    df_Downloaders.sort(['CompetitionId','UserId','LogDate'], ascending=[1, 1, 1], inplace=True, axis=0)\n",
      "    df_Downloaders.drop_duplicates(cols=['CompetitionId','UserId'], take_last=False, inplace=True)\n",
      "    \n",
      "    # Get counts using function .size()\n",
      "    by_comp_by_day = df_Downloaders.groupby(['CompetitionId','LogDate'])\n",
      "    df_InferredRulesAccepters = by_comp_by_day.size().to_frame()\n",
      "    # Munge the dataframe to be constructed like QueryResults1790\n",
      "    df_InferredRulesAccepters.reset_index(inplace=True)\n",
      "    df_InferredRulesAccepters.rename(columns={0:'RuleAccepted'}, inplace=True)\n",
      "\n",
      "    # Append to existing Submissions\n",
      "    df_RuleAccepters = pd.concat([df_RuleAccepters, df_InferredRulesAccepters], axis=0)\n",
      "\n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_RuleAccepters['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_RuleAccepters.LogDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "\n",
      "    # QueryResults has null CompetitionIds -> when user accepted sitewide Terms of Service\n",
      "    # remove null CompetitionIds\n",
      "    df_RuleAccepters = df_RuleAccepters[ df_RuleAccepters.CompetitionId.isnull() == False ]\n",
      "    df_RuleAccepters.sort(['CompetitionId','LogDate'], ascending=[1, 1], inplace=True, axis=0)\n",
      "\n",
      "\n",
      "    print \"Contains data for %d competitions,\" % len(np.unique(df_RuleAccepters.CompetitionId))\n",
      "    print \"   constraining to %d competitions under analysis\" % len(master_compids)\n",
      "    print \"   Calculating cumulative total of Accepters per day ...\"\n",
      "    \n",
      "    t0 = time()\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in master_compids:\n",
      "        operate = df_RuleAccepters.iloc[:,[1,3,2]][ (df_RuleAccepters.CompetitionId == c) ].set_index('DaySeq')\n",
      "        cumul = 0\n",
      "        for i in operate.index.values:\n",
      "            cumul = cumul + operate.xs(i,axis=0,copy=False)['RuleAccepted']\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    duration = time() - t0\n",
      "    print \"   ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    \n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulAccepted']).to_csv('./data/cleaned/rules_accepters.csv') \n",
      "        print \"\\n=  Clean output: /data/cleaned/rules_accepters.csv\\n\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/rules_accepters.csv   \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([3,])\n",
      "    operate = []\n",
      "    df_RuleAccepters = []\n",
      "    df_InferredRulesAccepters = []\n",
      "    df_Downloaders = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "\n",
      "# Load Forums table (1792)\n",
      "def clean1792(filename):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1792.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/forum_msg.csv\n",
      "      /cleaned/forum_msg_length.csv\"\"\"\n",
      "    \n",
      "    t00 = time()\n",
      "    try:\n",
      "        df_Forums = pd.read_csv(filename,header=0,usecols=[0,1,2,3,4,5,6,7],converters={'Message': lambda x: len(str(x).split()) })\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,4,9,10])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1792.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Join to get CompId\n",
      "    df_Forums = pd.merge(df_keys, df_Forums, left_on='X16', right_on='ForumId', how='inner')\n",
      "    df_Forums.rename(columns={'Message':'MessageLen'}, inplace=True)                \n",
      "    \n",
      "    # Sort and count Forum Messages\n",
      "    print \"Counting the forum messages per day ...\"\n",
      "    t0 = time()\n",
      "    \n",
      "    df_ForumMg = df_Forums.drop(['X16','Name','Name.1','ForumId','ForumTopicId','FlaggedCount'], axis=1)\n",
      "    df_ForumMg['PostDate'] = df_ForumMg.PostDate.map(lambda x:x[0:10])   # dumb down timestamp to just day, keeping it as String\n",
      "\n",
      "    # Now convert PostDate to a datetime\n",
      "    df_ForumMg['PostDate'] = pd.to_datetime(df_ForumMg.PostDate, utc=True)  \n",
      "    df_ForumMg['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_ForumMg.PostDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_ForumMg = df_ForumMg.drop(['PostDate'], axis=1) \n",
      "    \n",
      "    \n",
      "\n",
      "    df_ForumMg.set_index(['Id','ForumMessageId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    df_ForumMg.sort(['Id','ForumMessageId'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    # create a DataFrameGroupBy:\n",
      "    by_comp_by_day = df_ForumMg.groupby(['Id','DaySeq'])\n",
      "    \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_ForumMgCount = by_comp_by_day.size().to_frame().to_csv('./data/cleaned/forum_msg.csv')\n",
      "        df_ForumMgCount = []\n",
      "        print \"\\n=  Clean output: /data/cleaned/forum_msg.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/forum_msg.csv   \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Iterative calc on length of Forum Messages\n",
      "    print \"Calculating the average word-length per day (14-day rolling mean) ...\"\n",
      "    print \"   Contains data for %d competitions,\" % len(np.unique(df_ForumMg.Id))\n",
      "    print \"   constraining to %d competitions under analysis\" % len(master_compids)\n",
      "    t0 = time()\n",
      "    \n",
      "    measurem_array = np.empty([3,],dtype=float)\n",
      "    df_ForumMg.sort(['Id','DaySeq'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    \n",
      "    for c in master_compids:\n",
      "        subslice = df_ForumMg[df_ForumMg.Id == c]\n",
      "        if len(subslice) > 0:\n",
      "            minday = np.int(subslice.Day0.min())\n",
      "            maxday = np.int(subslice.DayEnd.max())\n",
      "            for i in range(minday, maxday+1):\n",
      "                get_2week_avg = subslice[(subslice.DaySeq <= i ) & (subslice.DaySeq >= i-14)]['MessageLen'].mean()\n",
      "                if pd.isnull(get_2week_avg) == True:\n",
      "                    get_2week_avg = 0\n",
      "                collectrow = [c, i, get_2week_avg]\n",
      "                measurem_array = np.vstack([measurem_array, collectrow])\n",
      "    collectrow = []\n",
      "    measurem_array = measurem_array[1:]  # delete first row artifact from creating an np.empty  \n",
      "    \n",
      "    try:\n",
      "        pd.DataFrame(data=measurem_array, columns=['CompetitionId','DaySeq','MessageLen2WeekTrailingAvg']).to_csv('./data/cleaned/forum_msg_length.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/forum_msg_length.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs \\n\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/forum_msg_length.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # TODO: ForumTopics - later if time permits\n",
      "    \n",
      "    ## Sort and count Forum Topics\n",
      "    #t0 = time()\n",
      "    #print \"Counting Forum topics per day...\"\n",
      "    #df_ForumTp = df_Forums.drop(['X16','Name','Name.1','MessageLen','FlaggedCount'], axis=1)\n",
      "    #      #df_ForumTp.set_index(['Id','ForumId','ForumTopicId'],drop=False,inplace=True, verify_integrity=True)\n",
      "    #df_ForumTp['PostDate'] = df_ForumTp.PostDate.map(lambda x:x[0:10])  # dumb down timestamp to just day\n",
      "    #df_ForumTp['PostDate'] = pd.to_datetime(df_ForumTp.PostDate, utc=True)\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_keys = []\n",
      "    df_ForumMgCount = []\n",
      "    df_Forums = []\n",
      "    df_ForumMg = []\n",
      "    by_comp_by_day = []\n",
      "    subslice = []\n",
      "    measurem_array = []\n",
      "\n",
      "    duration = time() - t00\n",
      "    print \"File done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Load Submissions tables (1810-1814, 1820)\n",
      "def clean1810(filename1=\"data/raw/QueryResults1810.csv\", filename2=\"data/raw/QueryResults1811.csv\", filename3=\"data/raw/QueryResults1812.csv\", filename4=\"data/raw/QueryResults1813.csv\", filename5=\"data/raw/QueryResults1814.csv\", filename6=\"data/raw/QueryResults1820.csv\"):\n",
      "    \"\"\" \n",
      "     ingests, cleans, and munges columns from /raw/QueryResults1810 - 1814.csv (leaderboard entries) \n",
      "     and QueryResults1820.csv (Prospect entries)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_globally.csv\n",
      "      /cleaned/submissions.csv\"\"\"\n",
      "\n",
      "    try:\n",
      "        df_Submissions = pd.concat([pd.read_csv(filename1, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename2, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename3, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename4, header=0, index_col=0),\n",
      "                                    pd.read_csv(filename5, header=0, index_col=0)],\n",
      "                                   axis=0)\n",
      "        df_Prospector = pd.read_csv(filename6, header=0, usecols=[1,2,3,4,6])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1810 - 1814.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2, filename3, filename4, filename5, filename6\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With valid submissions JOINED to team members, found %d entries\" % len(df_Submissions)\n",
      "    print \"Adding %d Prospector entries as 'submissions' ...\" % len(df_Prospector)\n",
      "    # Ingest prospect entries\n",
      "    # Set SubmId so high it could not collide with any Submissions table Id's\n",
      "    df_Prospector['SubmId']=df_Prospector.SubmId + 10000000\n",
      "    # Create fake TeamIds so they do not groupby to the same Null\n",
      "    df_Prospector['TeamId']=df_Prospector.UserId + df_Prospector.TotalVotes + 10000000\n",
      "    df_Prospector = df_Prospector.drop(['TotalVotes'], axis=1)\n",
      "    df_Prospector.rename(columns={'DateCreated':'DateSubmitted'}, inplace=True) \n",
      "    df_Prospector.set_index(['SubmId'],drop=True,inplace=True,verify_integrity=True)\n",
      "    # Append to existing Submissions\n",
      "    df_Submissions = pd.concat([df_Submissions, df_Prospector], axis=0)\n",
      "\n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_Submissions['DateSubmitted'] = df_Submissions.DateSubmitted.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_Submissions['DateSubmitted'] = pd.to_datetime(df_Submissions.DateSubmitted, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_Submissions['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_Submissions.DateSubmitted - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_Submissions = df_Submissions[ df_Submissions.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    # Output 1: Count of valid-submitting Users globally on the site\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_SubmUsersGlobal = df_Submissions.drop(['DateSubmitted','PublicScore','ScoreStatus','TeamId','CompetitionId'], axis=1)\n",
      "    # Set and sort on a new index to guarantee User falls with first submission ever\n",
      "    df_SubmUsersGlobal.set_index([df_SubmUsersGlobal.index,'UserId'],drop=False,inplace=True)\n",
      "    df_SubmUsersGlobal.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    \n",
      "    \n",
      "    print \"Calculate cumulative total of global Submitting Users per day:\"\n",
      "    print \"  Constraining to unique users ...\"\n",
      "    t0 = time()\n",
      "    # Constrain to unique Users, taking first day appears\n",
      "    df_SubmUsersGlobal.drop_duplicates(cols='UserId', take_last=False, inplace=True)\n",
      "    # Now set index to days, over which to iterate\n",
      "    df_SubmUsersGlobal.set_index('DaySeq', drop=True, inplace=True)\n",
      "    \n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([2,],dtype=float)\n",
      "    cumul = 0\n",
      "    for i in np.unique(df_SubmUsersGlobal.index.values):\n",
      "        cumul = cumul + df_SubmUsersGlobal.xs(i,axis=0,copy=False)['UserId'].size\n",
      "        cumul_array = np.vstack([cumul_array, [ i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['DaySeq','CumulUsers']).to_csv('./data/cleaned/users_globally.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_globally.csv \\n\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_globally.csv \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Output 2: Count of valid submissions per competition\n",
      "    print \"Calculate submissions made to competitions each day:\"\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_Submissions = df_Submissions.drop(['UserId'], axis=1)\n",
      "    df_Submissions.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Reduce to %d unique Submissions ...\" % len(df_Submissions)\n",
      "    \n",
      "    df_Submissions = df_Submissions.drop(['DateSubmitted','ScoreStatus','TeamId'], axis=1)\n",
      "    df_Submissions.sort(['CompetitionId','DaySeq'], ascending=[1, 1], inplace=True, axis=0)\n",
      "    print \"  Contains data for %d competitions, but\" % len(np.unique(df_Submissions.CompetitionId))\n",
      "    print \"  won't attempt to contrain these\"\n",
      "    \n",
      "    by_comp_by_day = df_Submissions.groupby(['CompetitionId','DaySeq'])\n",
      "    \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_SubmissionsCount = by_comp_by_day.size().to_frame().to_csv('./data/cleaned/submissions.csv')\n",
      "        df_SubmissionsCount = []\n",
      "        print \"\\n=  Clean output: /data/cleaned/submissions.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/submissions.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_SubmUsersGlobal = []\n",
      "    df_Submissions = []\n",
      "    by_comp_by_day = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "\n",
      "# Load Teams-Users table (1815)\n",
      "def clean1815(filename1=\"data/raw/QueryResults1815.csv\", filename2=\"data/raw/QueryResults1820.csv\"):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1815.csv \n",
      "     and and QueryResults1820.csv (Prospect entries)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/teams_users.csv\n",
      "      /cleaned/teams.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_TeamsUsers = pd.read_csv(filename1,header=0,usecols=[0,1,2,5,6])\n",
      "        df_Prospector = pd.read_csv(filename2, header=0, usecols=[1,2,3,4,6])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1815.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With teams JOINED to team members, found %d entries\" % len(df_TeamsUsers)\n",
      "    \n",
      "    # Use also QueryResults1820.csv -> in order to include Users on Prospect competitions\n",
      "    # constrain to unique userids per compid\n",
      "    df_Prospector.sort(['CompetitionId','UserId','DateCreated'], ascending=[1, 1, 1], inplace=True, axis=0)\n",
      "    df_Prospector.drop_duplicates(cols=['CompetitionId','UserId'], take_last=False, inplace=True)\n",
      "    print \"Also Interpreting %d Prospector entrants as 'team members'...\" % len(df_Prospector)\n",
      "    \n",
      "    # Ingest prospect entries\n",
      "    # Create fake TeamIds so they do not groupby to the same Null\n",
      "    df_Prospector['TeamId'] = df_Prospector.UserId + df_Prospector.TotalVotes + 10000000\n",
      "    df_Prospector = df_Prospector.drop(['TotalVotes','SubmId'], axis=1)\n",
      "    df_Prospector.rename(columns={'DateCreated':'ScoreFirstSubmittedDate'}, inplace=True) \n",
      "    # Append at bottom to existing TeamsUsers\n",
      "    df_TeamsUsers = pd.concat([df_TeamsUsers, df_Prospector], axis=0)\n",
      "\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_TeamsUsers['ScoreFirstSubmittedDate'] = df_TeamsUsers.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_TeamsUsers['ScoreFirstSubmittedDate'] = pd.to_datetime(df_TeamsUsers.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_TeamsUsers['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_TeamsUsers.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_TeamsUsers = df_TeamsUsers.drop(['ScoreFirstSubmittedDate'], axis=1) \n",
      "    \n",
      "    \n",
      "    # Output 1: Count of users on teams\n",
      "    \n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeamsUsers = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    print \"Calculate cumulative total of *users on teams* per day:\"\n",
      "    t0 = time()\n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeamsUsers.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeamsUsers.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeamsUsers.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeamsUsers.DaySeq[df_CumulTeamsUsers.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeamsUsers.xs([c,i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeamsUsers']).to_csv('./data/cleaned/teams_users.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/teams_users.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_users.csv   \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "        \n",
      "    # Output 2: Count of multiplayer teams\n",
      "    \n",
      "    try:\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "    except IOError:\n",
      "        print \" (>_<)  WARNING: Can't find input file: data/cleaned/foreignkeys.csv \\n\"\n",
      "        return False\n",
      "        \n",
      "    print \"Count the multiplayer teams who participated:\"\n",
      "    t0 = time()\n",
      "    # Join to get DayEnd\n",
      "    df_TeamsUsers = pd.merge(df_keys, df_TeamsUsers, left_on='Id', right_on='CompetitionId', how='inner')             \n",
      "      \n",
      "    # remove null CompetitionIds\n",
      "    print \"  Enforce that CompetitionId is not null ...\"\n",
      "    df_MultiTeams = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    # Drop unneeded columns\n",
      "    df_MultiTeams = df_MultiTeams.drop(['Ranking'], axis=1)\n",
      "    print \"  Counting members on each team ...\"\n",
      "    df_MultiTeams['Members'] = df_MultiTeams[['TeamId','CompetitionId']].groupby(['TeamId']).transform('count')\n",
      "    print \"  Dropping single player teams ...\"\n",
      "    df_MultiTeams = df_MultiTeams[df_MultiTeams.Members > 1]\n",
      "    df_MultiTeams = df_MultiTeams[df_MultiTeams.DaySeq <= df_MultiTeams.DayEnd]\n",
      "    df_MultiTeams = df_MultiTeams.drop(['Id','DayEnd','UserId'], axis=1)\n",
      "    df_MultiTeams.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Found %d multiplayer teams\" % len(df_MultiTeams)\n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    t0 = time()\n",
      "    by_comp = df_MultiTeams.groupby(['CompetitionId'])\n",
      "        \n",
      "    try:\n",
      "        # Get counts using function .size() -- counts even nulls\n",
      "        df_MultiTeamsCount = by_comp.size().to_frame().to_csv('./data/cleaned/teams_multiplayer.csv')\n",
      "        df_MultiTeamsCount = []\n",
      "        print \"\\n=  Clean output: /data/cleaned/teams_multiplayer.csv\"\n",
      "        duration = time() - t0\n",
      "        print \"   ::done in %fs \\n\" % duration\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_multiplayer.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "        \n",
      "        \n",
      "    # Output 3: Count of teams\n",
      "    \n",
      "    print \"Calculate cumulative total of teams per day:\"\n",
      "    t0 = time()\n",
      "    # remove null CompetitionIds\n",
      "    print \"  Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeams = df_TeamsUsers[ df_TeamsUsers.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    # Drop unneeded columns\n",
      "    df_CumulTeams = df_CumulTeams.drop(['UserId', 'Ranking'], axis=1)\n",
      "    df_CumulTeams.drop_duplicates(take_last=False, inplace=True)\n",
      "    print \"  Reduce to %d unique teams ...\" % len(df_CumulTeams)\n",
      "    \n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeams.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeams.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    \n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeams.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeams.DaySeq[df_CumulTeams.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeams.xs([c,i],axis=0,copy=False)['TeamId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeams']).to_csv('./data/cleaned/teams.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/teams.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams.csv   \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_TeamsUsers = []\n",
      "    df_MultiTeams = []\n",
      "    df_MultiTeamsCount = []\n",
      "    df_CumulTeamUsers = []\n",
      "    df_CumulTeams = []\n",
      "    df_keys = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Load Teams-Users-Bios table (1816)\n",
      "def clean1816(filename1=\"data/raw/QueryResults1816.csv\",filename2=\"data/raw/QueryResults1847.csv\"):\n",
      "    \"\"\" ingests, cleans, and munges columns from /raw/QueryResults1816.csv \n",
      "     and QueryResults1847.csv (Prospect users)\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/teams_users_bios.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_TeamsUsersBios = pd.read_csv(filename1,header=0,usecols=[0,1,2,4])\n",
      "        df_ProspectorBios = pd.read_csv(filename2, header=0, usecols=[1,2,3,4,5])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1816.csv and QueryResults1847.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    \n",
      "    print \"With teams JOINED to team members with bios, found %d entries\" % len(df_TeamsUsersBios)\n",
      "    # Ingest prospect users with bios (not yet unique UserIds)\n",
      "    # Set SubmId so high it could not collide with any Submissions table Id's\n",
      "    df_ProspectorBios['SubmId']=df_ProspectorBios.SubmId + 10000000\n",
      "    df_ProspectorBios = df_ProspectorBios.drop(['TotalVotes'], axis=1)\n",
      "    df_ProspectorBios.rename(columns={'DateCreated':'ScoreFirstSubmittedDate','SubmId':'Id'}, inplace=True) \n",
      "    # sort for first day of user, then drop duplicates\n",
      "    df_ProspectorBios.sort(['CompetitionId','UserId','ScoreFirstSubmittedDate'], ascending=[1, 1, 1], inplace=True, axis=0)\n",
      "    df_ProspectorBios.drop_duplicates(cols=['CompetitionId','UserId'], take_last=False, inplace=True)\n",
      "    print \"Adding %d Prospector users with bios to list ...\" % len(df_ProspectorBios)\n",
      "    # Append to existing Users with bios\n",
      "    df_TeamsUsersBios = pd.concat([df_TeamsUsersBios, df_ProspectorBios], axis=0)    \n",
      "\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"Force column type of date fields ...\"\n",
      "    df_TeamsUsersBios['ScoreFirstSubmittedDate'] = df_TeamsUsersBios.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_TeamsUsersBios['ScoreFirstSubmittedDate'] = pd.to_datetime(df_TeamsUsersBios.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_TeamsUsersBios['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_TeamsUsersBios.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_TeamsUsersBios = df_TeamsUsersBios.drop(['ScoreFirstSubmittedDate'], axis=1) \n",
      "    \n",
      "    # remove null CompetitionIds\n",
      "    print \"Enforce that CompetitionId is not null ...\"\n",
      "    df_CumulTeamsUsers = df_TeamsUsersBios[ df_TeamsUsersBios.CompetitionId.isnull() == False ]\n",
      "    \n",
      "    print \"Calculate cumulative total of *users on teams* per day:\"\n",
      "    t0 = time()\n",
      "    # Set index to compid, dayseq -> over which to iterate\n",
      "    df_CumulTeamsUsers.set_index(['CompetitionId','DaySeq'], drop=False, inplace=True)\n",
      "    df_CumulTeamsUsers.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    for c in np.unique(df_CumulTeamsUsers.CompetitionId.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CumulTeamsUsers.DaySeq[df_CumulTeamsUsers.CompetitionId == c].values):\n",
      "            cumul = cumul + df_CumulTeamsUsers.xs([c,i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['CompetitionId','DaySeq','CumulTeamsUsersBios']).to_csv('./data/cleaned/teams_users_bios.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/teams_users_bios.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/teams_users_bios.csv   \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "    \n",
      "    # Reclaim memory\n",
      "    cumul_array = np.empty([2,])\n",
      "    df_TeamsUsersBios = []\n",
      "    df_CumulTeamUsers = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Load Key Countries table (1808)\n",
      "def clean1808(filename=\"data/raw/QueryResults1808.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1808.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_countries.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1808.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count of users from key countries:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_CountCountries = pd.merge(df_keys, df_CountCountries, left_on='Id', right_on='CompetitionId', how='left')\n",
      "    df_CountCountries['CountUserIdsOnTeams'].fillna(value=0,inplace=True)\n",
      "    df_CountCountries = df_CountCountries.drop(['CompetitionId','DayEnd'], axis=1)\n",
      "    df_CountCountries.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_CountCountries.to_csv('./data/cleaned/users_countries.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_countries.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_countries.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Find Universe of users on submitting teams for Key Countries (1560)\n",
      "def clean1560(filename=\"data/raw/QueryResults1560.csv\"):\n",
      "    \"\"\" ingests, and munges columns from /raw/QueryResults1560.csv\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_globally_countries.csv\"\"\"\n",
      "\n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename, header=0)\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1560.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "\n",
      "    t00 = time()\n",
      "    print \"Calculate universe of users on teams for %d key countries:\" % len(np.unique(df_CountCountries.Country.values))\n",
      "    print \"  Geolocation is known for %d submitting teams users\" % len(df_CountCountries)\n",
      "    \n",
      "    # Convert text dates to UTC datetimes\n",
      "    print \"  Force column type of date field ...\"\n",
      "    df_CountCountries['ScoreFirstSubmittedDate'] = df_CountCountries.ScoreFirstSubmittedDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_CountCountries['ScoreFirstSubmittedDate'] = pd.to_datetime(df_CountCountries.ScoreFirstSubmittedDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_CountCountries['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_CountCountries.ScoreFirstSubmittedDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    \n",
      "    print \"  Constraining to unique users ...\"\n",
      "    # Set and sort on a new index to guarantee User falls with first submission ever\n",
      "    df_CountCountries.set_index(['UserId','DaySeq'],drop=False,inplace=True)\n",
      "    df_CountCountries.sort_index(axis=0, ascending=[1,1], inplace=True)\n",
      "    # Constrain to unique Users, taking first day appears\n",
      "    df_CountCountries.drop_duplicates(cols='UserId', take_last=False, inplace=True)\n",
      "    print \"  ::Reduced to %d unique submitting users\" % len(df_CountCountries)\n",
      "    # Now set index to days, over which to iterate\n",
      "    df_CountCountries.set_index(['Country','DaySeq'], drop=False, inplace=True)\n",
      "    df_CountCountries.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    \n",
      "    t0 = time()\n",
      "    print \"  Running calculation ...\"\n",
      "    cumul_array = np.empty([3,],dtype=float)\n",
      "    cumul = 0\n",
      "    for c in np.unique(df_CountCountries.Country.values):\n",
      "        cumul = 0\n",
      "        for i in np.unique(df_CountCountries.DaySeq[df_CountCountries.Country == c].values):\n",
      "            cumul = cumul + df_CountCountries.xs([c, i],axis=0,copy=False)['UserId'].size\n",
      "            cumul_array = np.vstack([cumul_array, [ c, i, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['Country','DaySeq','CumulUsers']).to_csv('./data/cleaned/users_globally_countries.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_globally_countries.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_globally_countries.csv \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True    \n",
      "\n",
      "\n",
      "# Load All Countries table (1809)\n",
      "def clean1809(filename=\"data/raw/QueryResults1809.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1809.csv \n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_countries_all.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_CountCountries = pd.read_csv(filename,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,10])\n",
      "        print\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1809.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count of all countries represented by CreatedIPaddr of users:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_CountCountries = pd.merge(df_keys, df_CountCountries, left_on='Id', right_on='CompetitionId', how='left')\n",
      "    df_CountCountries.fillna(value=0,inplace=True)\n",
      "    df_CountCountries = df_CountCountries.drop(['CompetitionId'], axis=1)\n",
      "    df_CountCountries.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_CountCountries.to_csv('./data/cleaned/users_countries_all.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_countries_all.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_countries_all.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_CountCountries = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Load LimitedParticipation table (1842)\n",
      "def clean1842(filename1=\"data/raw/QueryResults1842.csv\", filename2=\"data/raw/QueryResults1855.csv\"):\n",
      "    \"\"\" ingests and cleans columns from /raw/QueryResults1842.csv and data/raw/QueryResults1855.csv\n",
      "    \n",
      "    Outputs:\n",
      "      /cleaned/users_limited_entry.csv\n",
      "      /cleaned/users_masters_entry.csv\"\"\"\n",
      "    \n",
      "    try:\n",
      "        df_LimitedGroups = pd.read_csv(filename1,header=0)\n",
      "        df_MastersEntry = pd.read_csv(filename2,header=0)\n",
      "        df_keys = pd.read_csv(\"data/cleaned/foreignkeys.csv\",header=0, usecols=[0,7,8])\n",
      "        print \"\\n================================================================================\"\n",
      "        print \"cleaner for QueryResults1842.csv and QueryResults1855.csv\"\n",
      "        print \"________________________________________________________________________________\"\n",
      "    except IOError:\n",
      "        print \"Can't find input file: \", filename1, filename2\n",
      "        print\n",
      "        printhelp()\n",
      "        return False\n",
      "    \n",
      "    # Data Munging\n",
      "    t00 = time()\n",
      "    print \"Count maximum users in limited entry competitions:\"\n",
      "    print \"   constraining to %d competitions under analysis ...\" % len(master_compids)\n",
      "    # Join to get CompId\n",
      "    df_keys.X23[pd.isnull(df_keys.X23) == True] = df_keys.X22\n",
      "    df_LimitedGroups = pd.merge(df_LimitedGroups, df_keys, left_on='GroupId', right_on='X23', how='inner')\n",
      "    df_LimitedGroups = df_LimitedGroups.drop(['GroupTypeId','X22','X23'], axis=1)\n",
      "    df_LimitedGroups.set_index(['Id'],drop=True,inplace=True)\n",
      "    try:\n",
      "        df_LimitedGroups.to_csv('./data/cleaned/users_limited_entry.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_limited_entry.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_limited_entry.csv  \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # Treat Masters group as separate cumul total across all Masters competitions\n",
      "    # Convert text dates to UTC datetimes\n",
      "    t0 = time()\n",
      "    print \"Calculate cumulative total of unique masters users per day:\"\n",
      "    print \"   Force column type of date fields ...\"\n",
      "    df_MastersEntry['LogDate'] = df_MastersEntry.LogDate.map(lambda x:x[0:10])   # dumb down timestamp to just day\n",
      "    df_MastersEntry['LogDate'] = pd.to_datetime(df_MastersEntry.LogDate, utc=True) \n",
      "    # Convert dates to sequential day from 1/1/2010\n",
      "    df_MastersEntry['DaySeq'] = np.round(pd.to_timedelta( \n",
      "                                                  (df_MastersEntry.LogDate - origindate),\n",
      "                                                  unit='D').astype(np.timedelta64) / 86400 / 1000000000)\n",
      "    df_MastersEntry = df_MastersEntry.drop(['LogDate'], axis=1) \n",
      "    # remove null CompetitionIds\n",
      "    print \"   Enforce that CompetitionId is not null ...\"\n",
      "    df_MastersEntry = df_MastersEntry[ df_MastersEntry.CompetitionId.isnull() == False ]\n",
      "    # Set index -> over which to iterate\n",
      "    df_MastersEntry.set_index(['DaySeq'], drop=False, inplace=True)\n",
      "    df_MastersEntry.sort_index(axis=0, ascending=True, inplace=True)\n",
      "    df_MastersEntry.drop_duplicates(cols=['Id'], take_last=False, inplace=True)\n",
      "    print \"   Running calculation ...\"\n",
      "    df_CumulEntry = df_MastersEntry.groupby(df_MastersEntry.index,axis=0,as_index=False).size().to_frame()\n",
      "    df_CumulEntry.rename(columns={0:'TotalEntrants'}, inplace=True)\n",
      "    cumul_array = np.empty([2,],dtype=float)\n",
      "    cumul = 0\n",
      "    for ds in np.unique(df_CumulEntry.index.values):\n",
      "        cumul = cumul + df_CumulEntry.loc[ds]['TotalEntrants']\n",
      "        cumul_array = np.vstack([cumul_array, [ ds, cumul ]])\n",
      "    \n",
      "    duration = time() - t0\n",
      "    print \"  ::done in %fs\" % duration\n",
      "    \n",
      "    cumul_array = cumul_array[1:]  # delete first row artifact from creating an np.empty\n",
      "    try:\n",
      "        pd.DataFrame(data=cumul_array, columns=['DaySeq','TotalEntrants']).to_csv('./data/cleaned/users_masters_entry.csv')\n",
      "        print \"\\n=  Clean output: /data/cleaned/users_masters_entry.csv\"\n",
      "    except IOError:\n",
      "        print \"WARNING: Failed to write out file: data/cleaned/users_masters_entry.csv \u00af\\_(\u30c4)_/\u00af \\n\"\n",
      "        return False\n",
      "\n",
      "    # Reclaim memory\n",
      "    df_LimitedGroups = []\n",
      "    df_MastersEntry = []\n",
      "    df_CumulEntry = []\n",
      "    \n",
      "    duration = time() - t00\n",
      "    print \"\\nFile done in %fs\" % duration\n",
      "    return True\n",
      "\n",
      "# Okay, you're ready. Get to cleaning!\n",
      "\n",
      "duration = time() - t000\n",
      "print \"Start:\"\n",
      "print \"Numpy version\", np.__version__\n",
      "print \"Pandas version\", pd.__version__\n",
      "print \"Code loaded in %fs \\n\" % duration\n",
      "okay = True\n",
      "\n",
      "# Clean Competitions table\n",
      "codenames, master_compids = clean1784(\"data/raw/QueryResults1784.csv\", visualize=True)   \n",
      "if okay == True:  # Clean RulesAccepters\n",
      "    okay = clean1790(\"data/raw/QueryResults17901.csv\",\"data/raw/QueryResults1833.csv\")    \n",
      "if okay == True:  # Clean Forums\n",
      "    okay = clean1792(\"data/raw/QueryResults1792.csv\")    \n",
      "if okay == True:  # Clean Submissions & Prospectors\n",
      "    okay = clean1810(\"data/raw/QueryResults1810.csv\",\n",
      "                     \"data/raw/QueryResults1811.csv\",\n",
      "                     \"data/raw/QueryResults1812.csv\",\n",
      "                     \"data/raw/QueryResults1813.csv\",\n",
      "                     \"data/raw/QueryResults1814.csv\",\n",
      "                     \"data/raw/QueryResults1820.csv\")\n",
      "if okay == True:  # Clean Teams-Users\n",
      "    okay = clean1815(\"data/raw/QueryResults1815.csv\",\"data/raw/QueryResults1820.csv\")    \n",
      "if okay == True:  # Clean Teams-Users with Bios\n",
      "    okay = clean1816(\"data/raw/QueryResults1816.csv\",\"data/raw/QueryResults1847.csv\")    \n",
      "if okay == True:  # Clean T12 countries\n",
      "    okay = clean1808(\"data/raw/QueryResults1808.csv\")     \n",
      "if okay == True:  # Clean T12 countries globally\n",
      "    okay = clean1560(\"data/raw/QueryResults1560.csv\")     \n",
      "if okay == True:  # Clean countries All\n",
      "    okay = clean1809(\"data/raw/QueryResults1809.csv\")     \n",
      "if okay == True:  # Clean Limited partic groups\n",
      "    okay = clean1842(\"data/raw/QueryResults1842.csv\",\"data/raw/QueryResults1855.csv\")  \n",
      "if okay == True:\n",
      "    duration = time() - t000\n",
      "    print \"\\n==============================================\"\n",
      "    print \"DATA_CLEANER COMPLETE! Finished in %fs    (\u00b4\u25bd\uff40)=b\" % duration\n",
      "    print \"==============================================\"\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Start:\n",
        "Numpy version 1.8.0\n",
        "Pandas version 0.13.1\n",
        "Code loaded in 0.023989s \n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1784.csv\n",
        "________________________________________________________________________________\n",
        "Masking feature names from raw data ...\n",
        "Flatten feature X17 to values {1 or 3} ...\n",
        "Force column type of boolean and date fields ...\n",
        "Checking for valid dates:\n",
        "   filling 54 missing dates  (\u2299\u2026\u2299 ) ...\n",
        "   found 5 questionable dates relative to Launch  (\u2299\u2026\u2299 ) ...\n",
        "   found 8 more questionable dates. Making assumptions  (\u25ce_\u25ce;)  ...\n",
        "   ::done in 0.023266s\n",
        "Today is  2014-05-21\n",
        "QueryResults have timestamp of  2014-05-09\n",
        "Creating new time measurements from datestamps ...\n",
        "  ::done in 0.030073s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Dropping competitions that are too young to analyze ...\n",
        "\n",
        "365 competitions are remaining for analysis\n",
        "Saving future JOIN tables ...\n",
        "=  Clean output: /data/cleaned/foreignkeys.csv \n",
        "\n",
        "=  Vis Output: /data/vis/features1_histograms.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 1.600218s\n",
        "=  Vis Output: /data/vis/features1_outliers.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 0.655299s\n",
        "=  Clean output: /data/cleaned/comps.csv \n",
        "\n",
        "File done in 2.419181s\n",
        "Can't find input file:  data/raw/QueryResults17901.csv data/raw/QueryResults1833.csv\n",
        "\n",
        "\u300c(\u00b0\u30d8\u00b0)  Oops: data_cleaner expects certain (non-public) files in /data/raw/ :\n",
        "\n",
        "  Prerequisite files: \n",
        "  - QueryResults1560.csv  from https://sql.kaggle.com/kaggle/query/1560/\n",
        "  - QueryResults1784.csv  from https://sql.kaggle.com/kaggle/query/1784/\n",
        "  - QueryResults1790.csv  from https://sql.kaggle.com/kaggle/query/1790/\n",
        "  - QueryResults1792.csv  from https://sql.kaggle.com/kaggle/query/1792/\n",
        "  - QueryResults1807.csv  from https://sql.kaggle.com/kaggle/query/1807/\n",
        "  - QueryResults1808.csv  from https://sql.kaggle.com/kaggle/query/1808/\n",
        "  - QueryResults1809.csv  from https://sql.kaggle.com/kaggle/query/1809/\n",
        "  - QueryResults1810.csv  from https://sql.kaggle.com/kaggle/query/1810/\n",
        "  - QueryResults1811.csv  from https://sql.kaggle.com/kaggle/query/1811/\n",
        "  - QueryResults1812.csv  from https://sql.kaggle.com/kaggle/query/1812/\n",
        "  - QueryResults1813.csv  from https://sql.kaggle.com/kaggle/query/1813/\n",
        "  - QueryResults1814.csv  from https://sql.kaggle.com/kaggle/query/1814/\n",
        "  - QueryResults1815.csv  from https://sql.kaggle.com/kaggle/query/1815/\n",
        "  - QueryResults1816.csv  from https://sql.kaggle.com/kaggle/query/1816/\n",
        "  - QueryResults1820.csv  from https://sql.kaggle.com/kaggle/query/1820/\n",
        "  - QueryResults1833.csv  from https://sql.kaggle.com/kaggle/query/1833/\n",
        "  - QueryResults1842.csv  from https://sql.kaggle.com/kaggle/query/1842/\n",
        "  - QueryResults1855.csv  from https://sql.kaggle.com/kaggle/query/1855/\n",
        "\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1792.csv\n",
        "________________________________________________________________________________\n",
        "Counting the forum messages per day ...\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 0.661913s\n",
        "Calculating the average word-length per day (14-day rolling mean) ...\n",
        "   Contains data for 199 competitions,\n",
        "   constraining to 365 competitions under analysis\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg_length.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 24.990889s \n",
        "\n",
        "File done in 26.515894s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1810 - 1814.csv\n",
        "________________________________________________________________________________\n",
        "With valid submissions JOINED to team members, found 654922 entries\n",
        "Adding 596 Prospector entries as 'submissions' ...\n",
        "Force column type of date fields ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of global Submitting Users per day:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Constraining to unique users ...\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 2.939948s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally.csv \n",
        "\n",
        "Calculate submissions made to competitions each day:\n",
        "  Reduce to 452818 unique Submissions ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Contains data for 442 competitions, but"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  won't attempt to contrain these\n",
        "\n",
        "=  Clean output: /data/cleaned/submissions.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 10.215624s\n",
        "\n",
        "File done in 20.784804s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1815.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members, found 47021 entries\n",
        "Also Interpreting 509 Prospector entrants as 'team members'...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 8.913379s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count the multiplayer teams who participated:\n",
        "  Enforce that CompetitionId is not null ...\n",
        "  Counting members on each team ...\n",
        "  Dropping single player teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Found 3322 multiplayer teams\n",
        "  ::done in 24.632317s\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_multiplayer.csv\n",
        "   ::done in 0.007691s \n",
        "\n",
        "Calculate cumulative total of teams per day:\n",
        "  Enforce that CompetitionId is not null ...\n",
        "  Reduce to 35151 unique teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 8.073949s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "File done in 42.375651s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1816.csv and QueryResults1847.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members with bios, found 10014 entries\n",
        "Adding 168 Prospector users with bios to list ...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ...\n",
        "  ::done in 3.542327s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users_bios.csv\n",
        "\n",
        "File done in 3.694410s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1808.csv\n",
        "________________________________________________________________________________\n",
        "Count of users from key countries:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_countries.csv\n",
        "\n",
        "File done in 0.010800s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1560.csv\n",
        "________________________________________________________________________________\n",
        "Calculate universe of users on teams for 12 key countries:\n",
        "  Geolocation is known for 36038 submitting teams users\n",
        "  Force column type of date field ...\n",
        "  Constraining to unique users ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::Reduced to 21596 unique submitting users"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ...\n",
        "  ::done in 4.589344s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally_countries.csv\n",
        "\n",
        "File done in 5.322098s\n",
        "\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1809.csv\n",
        "________________________________________________________________________________\n",
        "Count of all countries represented by CreatedIPaddr of users:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_countries_all.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "File done in 0.024625s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1842.csv and QueryResults1855.csv\n",
        "________________________________________________________________________________\n",
        "Count maximum users in limited entry competitions:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_limited_entry.csv\n",
        "Calculate cumulative total of unique masters users per day:\n",
        "   Force column type of date fields ...\n",
        "   Enforce that CompetitionId is not null ...\n",
        "   Running calculation ...\n",
        "  ::done in 0.037099s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_masters_entry.csv\n",
        "\n",
        "File done in 0.049658s\n",
        "Can't find input file:  data/raw/QueryResults17901.csv data/raw/QueryResults1833.csv\n",
        "\n",
        "\u300c(\u00b0\u30d8\u00b0)  Oops: data_cleaner expects certain (non-public) files in /data/raw/ :\n",
        "\n",
        "  Prerequisite files: \n",
        "  - QueryResults1560.csv  from https://sql.kaggle.com/kaggle/query/1560/\n",
        "  - QueryResults1784.csv  from https://sql.kaggle.com/kaggle/query/1784/\n",
        "  - QueryResults1790.csv  from https://sql.kaggle.com/kaggle/query/1790/\n",
        "  - QueryResults1792.csv  from https://sql.kaggle.com/kaggle/query/1792/\n",
        "  - QueryResults1807.csv  from https://sql.kaggle.com/kaggle/query/1807/\n",
        "  - QueryResults1808.csv  from https://sql.kaggle.com/kaggle/query/1808/\n",
        "  - QueryResults1809.csv  from https://sql.kaggle.com/kaggle/query/1809/\n",
        "  - QueryResults1810.csv  from https://sql.kaggle.com/kaggle/query/1810/\n",
        "  - QueryResults1811.csv  from https://sql.kaggle.com/kaggle/query/1811/\n",
        "  - QueryResults1812.csv  from https://sql.kaggle.com/kaggle/query/1812/\n",
        "  - QueryResults1813.csv  from https://sql.kaggle.com/kaggle/query/1813/\n",
        "  - QueryResults1814.csv  from https://sql.kaggle.com/kaggle/query/1814/\n",
        "  - QueryResults1815.csv  from https://sql.kaggle.com/kaggle/query/1815/\n",
        "  - QueryResults1816.csv  from https://sql.kaggle.com/kaggle/query/1816/\n",
        "  - QueryResults1820.csv  from https://sql.kaggle.com/kaggle/query/1820/\n",
        "  - QueryResults1833.csv  from https://sql.kaggle.com/kaggle/query/1833/\n",
        "  - QueryResults1842.csv  from https://sql.kaggle.com/kaggle/query/1842/\n",
        "  - QueryResults1855.csv  from https://sql.kaggle.com/kaggle/query/1855/\n",
        "\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1792.csv\n",
        "________________________________________________________________________________\n",
        "Counting the forum messages per day ...\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 0.659680s\n",
        "Calculating the average word-length per day (14-day rolling mean) ...\n",
        "   Contains data for 199 competitions,\n",
        "   constraining to 365 competitions under analysis\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg_length.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 24.937733s \n",
        "\n",
        "File done in 26.365637s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1810 - 1814.csv\n",
        "________________________________________________________________________________\n",
        "With valid submissions JOINED to team members, found 654922 entries\n",
        "Adding 596 Prospector entries as 'submissions' ...\n",
        "Force column type of date fields ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of global Submitting Users per day:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Constraining to unique users ...\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 2.488254s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally.csv \n",
        "\n",
        "Calculate submissions made to competitions each day:\n",
        "  Reduce to 452818 unique Submissions ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Contains data for 442 competitions, but"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  won't attempt to contrain these\n",
        "\n",
        "=  Clean output: /data/cleaned/submissions.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 9.504360s\n",
        "\n",
        "File done in 20.208136s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1815.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members, found 47021 entries\n",
        "Also Interpreting 509 Prospector entrants as 'team members'...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 9.380112s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count the multiplayer teams who participated:\n",
        "  Enforce that CompetitionId is not null ...\n",
        "  Counting members on each team ...\n",
        "  Dropping single player teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Found 3322 multiplayer teams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 25.039401s\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_multiplayer.csv\n",
        "   ::done in 0.003076s \n",
        "\n",
        "Calculate cumulative total of teams per day:\n",
        "  Enforce that CompetitionId is not null ...\n",
        "  Reduce to 35151 unique teams ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ...\n",
        "  ::done in 7.722541s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams.csv\n",
        "\n",
        "File done in 43.023506s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1816.csv and QueryResults1847.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members with bios, found 10014 entries\n",
        "Adding 168 Prospector users with bios to list ...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ...\n",
        "  ::done in 3.965128s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/teams_users_bios.csv\n",
        "\n",
        "File done in 4.117909s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1808.csv\n",
        "________________________________________________________________________________\n",
        "Count of users from key countries:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_countries.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "File done in 0.011557s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1560.csv\n",
        "________________________________________________________________________________\n",
        "Calculate universe of users on teams for 12 key countries:\n",
        "  Geolocation is known for 36038 submitting teams users\n",
        "  Force column type of date field ...\n",
        "  Constraining to unique users ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::Reduced to 21596 unique submitting users"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 5.000073s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally_countries.csv\n",
        "\n",
        "File done in 6.044274s\n",
        "\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1809.csv\n",
        "________________________________________________________________________________\n",
        "Count of all countries represented by CreatedIPaddr of users:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_countries_all.csv\n",
        "\n",
        "File done in 0.004701s\n",
        "\n",
        "================================================================================\n",
        "cleaner for QueryResults1842.csv and QueryResults1855.csv\n",
        "________________________________________________________________________________\n",
        "Count maximum users in limited entry competitions:\n",
        "   constraining to 365 competitions under analysis ...\n",
        "\n",
        "=  Clean output: /data/cleaned/users_limited_entry.csv\n",
        "Calculate cumulative total of unique masters users per day:\n",
        "   Force column type of date fields ...\n",
        "   Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   Running calculation ...\n",
        "  ::done in 0.029455s\n",
        "\n",
        "=  Clean output: /data/cleaned/users_masters_entry.csv\n",
        "\n",
        "File done in 0.049325s\n",
        "Can't find input file:  data/raw/QueryResults17901.csv data/raw/QueryResults1833.csv\n",
        "\n",
        "\u300c(\u00b0\u30d8\u00b0)  Oops: data_cleaner expects certain (non-public) files in /data/raw/ :\n",
        "\n",
        "  Prerequisite files: \n",
        "  - QueryResults1560.csv  from https://sql.kaggle.com/kaggle/query/1560/\n",
        "  - QueryResults1784.csv  from https://sql.kaggle.com/kaggle/query/1784/\n",
        "  - QueryResults1790.csv  from https://sql.kaggle.com/kaggle/query/1790/\n",
        "  - QueryResults1792.csv  from https://sql.kaggle.com/kaggle/query/1792/\n",
        "  - QueryResults1807.csv  from https://sql.kaggle.com/kaggle/query/1807/\n",
        "  - QueryResults1808.csv  from https://sql.kaggle.com/kaggle/query/1808/\n",
        "  - QueryResults1809.csv  from https://sql.kaggle.com/kaggle/query/1809/\n",
        "  - QueryResults1810.csv  from https://sql.kaggle.com/kaggle/query/1810/\n",
        "  - QueryResults1811.csv  from https://sql.kaggle.com/kaggle/query/1811/\n",
        "  - QueryResults1812.csv  from https://sql.kaggle.com/kaggle/query/1812/\n",
        "  - QueryResults1813.csv  from https://sql.kaggle.com/kaggle/query/1813/\n",
        "  - QueryResults1814.csv  from https://sql.kaggle.com/kaggle/query/1814/\n",
        "  - QueryResults1815.csv  from https://sql.kaggle.com/kaggle/query/1815/\n",
        "  - QueryResults1816.csv  from https://sql.kaggle.com/kaggle/query/1816/\n",
        "  - QueryResults1820.csv  from https://sql.kaggle.com/kaggle/query/1820/\n",
        "  - QueryResults1833.csv  from https://sql.kaggle.com/kaggle/query/1833/\n",
        "  - QueryResults1842.csv  from https://sql.kaggle.com/kaggle/query/1842/\n",
        "  - QueryResults1855.csv  from https://sql.kaggle.com/kaggle/query/1855/\n",
        "\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1792.csv\n",
        "________________________________________________________________________________\n",
        "Counting the forum messages per day ...\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 0.470074s\n",
        "Calculating the average word-length per day (14-day rolling mean) ...\n",
        "   Contains data for 199 competitions,\n",
        "   constraining to 365 competitions under analysis\n",
        "\n",
        "=  Clean output: /data/cleaned/forum_msg_length.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 26.978525s \n",
        "\n",
        "File done in 28.207076s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1810 - 1814.csv\n",
        "________________________________________________________________________________\n",
        "With valid submissions JOINED to team members, found 654922 entries\n",
        "Adding 596 Prospector entries as 'submissions' ...\n",
        "Force column type of date fields ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of global Submitting Users per day:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Constraining to unique users ...\n",
        "  Running calculation ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  ::done in 3.282838s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "=  Clean output: /data/cleaned/users_globally.csv \n",
        "\n",
        "Calculate submissions made to competitions each day:\n",
        "  Reduce to 452818 unique Submissions ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Contains data for 442 competitions, but"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  won't attempt to contrain these\n",
        "\n",
        "=  Clean output: /data/cleaned/submissions.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   ::done in 10.393134s\n",
        "\n",
        "File done in 21.907826s\n",
        "\n",
        "================================================================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cleaner for QueryResults1815.csv\n",
        "________________________________________________________________________________\n",
        "With teams JOINED to team members, found 47021 entries\n",
        "Also Interpreting 509 Prospector entrants as 'team members'...\n",
        "Force column type of date fields ...\n",
        "Enforce that CompetitionId is not null ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Calculate cumulative total of *users on teams* per day:\n",
        "  Running calculation ..."
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}